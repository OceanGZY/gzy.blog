{"title":"Bert-TF-2.6修改-自调适配版","slug":"bert-tf2-6","date":"2021-08-01T15:57:08.000Z","updated":"2022-09-30T06:56:37.166Z","comments":true,"path":"api/articles/bert-tf2-6.json","excerpt":null,"covers":["https://oceaneyes.top/img/zhishigroup.jpg","https://oceaneyes.top/img/alipay.jpg","https://oceaneyes.top/img/wechatpay.jpg"],"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"bert-tf-2.6修改-自调适配版\">Bert-TF-2.6修改-自调适配版</h1>\n<h4 id=\"背景\">背景</h4>\n<p>bert开源版适配的为tf1版本，当机器的tf环境为2以上版本时，会出现各种异常。</p>\n<p>因此我根据tf的函数库，进行了bert适配tf2.6的修改适配。</p>\n<h4 id=\"代码\">代码</h4>\n<h5 id=\"run_classifier.py\">run_classifier.py</h5>\n<pre class=\"python3\"><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport os\nimport modeling\nimport optimization\nimport tokenization\nimport tensorflow as tf\nfrom absl import flags\nfrom absl import app\nimport pickle\n\n# flags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    &quot;data_dir&quot;, None,\n    &quot;The input data dir. Should contain the .tsv files (or other data files) &quot;\n    &quot;for the task.&quot;)\n\nflags.DEFINE_string(\n    &quot;bert_config_file&quot;, None,\n    &quot;The config json file corresponding to the pre-trained BERT model. &quot;\n    &quot;This specifies the model architecture.&quot;)\n\nflags.DEFINE_string(&quot;task_name&quot;, None, &quot;The name of the task to train.&quot;)\n\nflags.DEFINE_string(&quot;vocab_file&quot;, None,\n                    &quot;The vocabulary file that the BERT model was trained on.&quot;)\n\nflags.DEFINE_string(\n    &quot;output_dir&quot;, None,\n    &quot;The output directory where the model checkpoints will be written.&quot;)\n\nflags.DEFINE_string(\n    &quot;trans_model_dir&quot;, None,\n    &quot;The trans_model_dir directory where the model will be written.&quot;)\n\n\n## Other parameters\n\nflags.DEFINE_string(\n    &quot;init_checkpoint&quot;, None,\n    &quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;)\n\nflags.DEFINE_bool(\n    &quot;do_lower_case&quot;, True,\n    &quot;Whether to lower case the input text. Should be True for uncased &quot;\n    &quot;models and False for cased models.&quot;)\n\nflags.DEFINE_integer(\n    &quot;max_seq_length&quot;, 128,\n    &quot;The maximum total input sequence length after WordPiece tokenization. &quot;\n    &quot;Sequences longer than this will be truncated, and sequences shorter &quot;\n    &quot;than this will be padded.&quot;)\n\nflags.DEFINE_bool(&quot;do_train&quot;, False, &quot;Whether to run training.&quot;)\n\nflags.DEFINE_bool(&quot;do_eval&quot;, False, &quot;Whether to run eval on the dev set.&quot;)\n\nflags.DEFINE_bool(\n    &quot;do_predict&quot;, False,\n    &quot;Whether to run the model in inference mode on the test set.&quot;)\n\nflags.DEFINE_integer(&quot;train_batch_size&quot;, 32, &quot;Total batch size for training.&quot;)\n\nflags.DEFINE_integer(&quot;eval_batch_size&quot;, 8, &quot;Total batch size for eval.&quot;)\n\nflags.DEFINE_integer(&quot;predict_batch_size&quot;, 8, &quot;Total batch size for predict.&quot;)\n\nflags.DEFINE_float(&quot;learning_rate&quot;, 5e-5, &quot;The initial learning rate for Adam.&quot;)\n\nflags.DEFINE_float(&quot;num_train_epochs&quot;, 3.0,\n                   &quot;Total number of training epochs to perform.&quot;)\n\nflags.DEFINE_float(\n    &quot;warmup_proportion&quot;, 0.1,\n    &quot;Proportion of training to perform linear learning rate warmup for. &quot;\n    &quot;E.g., 0.1 = 10% of training.&quot;)\n\nflags.DEFINE_integer(&quot;save_checkpoints_steps&quot;, 1000,\n                     &quot;How often to save the model checkpoint.&quot;)\n\nflags.DEFINE_integer(&quot;iterations_per_loop&quot;, 1000,\n                     &quot;How many steps to make in each estimator call.&quot;)\n\nflags.DEFINE_bool(&quot;use_tpu&quot;, False, &quot;Whether to use TPU or GPU/CPU.&quot;)\n\nflags.DEFINE_string(\n    &quot;tpu_name&quot;, None,\n    &quot;The Cloud TPU to use for training. This should be either the name &quot;\n    &quot;used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 &quot;\n    &quot;url.&quot;)\n\nflags.DEFINE_string(\n    &quot;tpu_zone&quot;, None,\n    &quot;[Optional] GCE zone where the Cloud TPU is located in. If not &quot;\n    &quot;specified, we will attempt to automatically detect the GCE project from &quot;\n    &quot;metadata.&quot;)\n\nflags.DEFINE_string(\n    &quot;gcp_project&quot;, None,\n    &quot;[Optional] Project name for the Cloud TPU-enabled project. If not &quot;\n    &quot;specified, we will attempt to automatically detect the GCE project from &quot;\n    &quot;metadata.&quot;)\n\nflags.DEFINE_string(&quot;master&quot;, None, &quot;[Optional] TensorFlow master URL.&quot;)\n\nflags.DEFINE_integer(\n    &quot;num_tpu_cores&quot;, 8,\n    &quot;Only used if `use_tpu` is True. Total number of TPU cores to use.&quot;)\n\n\nclass InputExample(object):\n  &quot;&quot;&quot;A single training/test example for simple sequence classification.&quot;&quot;&quot;\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    &quot;&quot;&quot;Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    &quot;&quot;&quot;\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass PaddingInputExample(object):\n  &quot;&quot;&quot;Fake example so the num input examples is a multiple of the batch size.\n\n  When running eval/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won&#39;t be generated.\n\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  &quot;&quot;&quot;\n\n\nclass InputFeatures(object):\n  &quot;&quot;&quot;A single set of features of data.&quot;&quot;&quot;\n\n  def __init__(self,\n               input_ids,\n               input_mask,\n               segment_ids,\n               label_id,\n               is_real_example=True):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.segment_ids = segment_ids\n    self.label_id = label_id\n    self.is_real_example = is_real_example\n\n\nclass DataProcessor(object):\n  &quot;&quot;&quot;Base class for data converters for sequence classification data sets.&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for the train set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for the dev set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for prediction.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_labels(self):\n    &quot;&quot;&quot;Gets the list of labels for this data set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    &quot;&quot;&quot;Reads a tab separated value file.&quot;&quot;&quot;\n    with tf.io.gfile.GFile(input_file, &quot;r&quot;) as f:\n      reader = csv.reader(f, delimiter=&quot;\\t&quot;, quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\nclass XnliProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the XNLI data set.&quot;&quot;&quot;\n\n  def __init__(self):\n    self.language = &quot;zh&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    lines = self._read_tsv(\n        os.path.join(data_dir, &quot;multinli&quot;,\n                     &quot;multinli.train.%s.tsv&quot; % self.language))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;train-%d&quot; % (i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[2])\n      if label == tokenization.convert_to_unicode(&quot;contradictory&quot;):\n        label = tokenization.convert_to_unicode(&quot;contradiction&quot;)\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    lines = self._read_tsv(os.path.join(data_dir, &quot;xnli.dev.tsv&quot;))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;dev-%d&quot; % (i)\n      language = tokenization.convert_to_unicode(line[0])\n      if language != tokenization.convert_to_unicode(self.language):\n        continue\n      text_a = tokenization.convert_to_unicode(line[6])\n      text_b = tokenization.convert_to_unicode(line[7])\n      label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;contradiction&quot;, &quot;entailment&quot;, &quot;neutral&quot;]\n\n\nclass MnliProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the MultiNLI data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev_matched.tsv&quot;)),\n        &quot;dev_matched&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test_matched.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;contradiction&quot;, &quot;entailment&quot;, &quot;neutral&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, tokenization.convert_to_unicode(line[0]))\n      text_a = tokenization.convert_to_unicode(line[8])\n      text_b = tokenization.convert_to_unicode(line[9])\n      if set_type == &quot;test&quot;:\n        label = &quot;contradiction&quot;\n      else:\n        label = tokenization.convert_to_unicode(line[-1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass MrpcProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the MRPC data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;0&quot;, &quot;1&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[3])\n      text_b = tokenization.convert_to_unicode(line[4])\n      if set_type == &quot;test&quot;:\n        label = &quot;0&quot;\n      else:\n        label = tokenization.convert_to_unicode(line[0])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass ColaProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the CoLA data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;0&quot;, &quot;1&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      # Only the test set has a header\n      if set_type == &quot;test&quot; and i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, i)\n      if set_type == &quot;test&quot;:\n        text_a = tokenization.convert_to_unicode(line[1])\n        label = &quot;0&quot;\n      else:\n        text_a = tokenization.convert_to_unicode(line[3])\n        label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  &quot;&quot;&quot;Converts a single `InputExample` into a single `InputFeatures`.&quot;&quot;&quot;\n\n  if isinstance(example, PaddingInputExample):\n    return InputFeatures(\n        input_ids=[0] * max_seq_length,\n        input_mask=[0] * max_seq_length,\n        segment_ids=[0] * max_seq_length,\n        label_id=0,\n        is_real_example=False)\n\n  label_map = &#123;&#125;\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  output_label2id_file = os.path.join(FLAGS.trans_model_dir, &quot;label2id.pkl&quot;)\n  if not os.path.exists(output_label2id_file):\n    with open(output_label2id_file, &#39;wb&#39;) as w:\n        pickle.dump(label_map, w)\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with &quot;- 3&quot;\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with &quot;- 2&quot;\n    if len(tokens_a) &gt; max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where &quot;type_ids&quot; are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as the &quot;sentence vector&quot;. Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(&quot;[CLS]&quot;)\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(&quot;[SEP]&quot;)\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(&quot;[SEP]&quot;)\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) &lt; max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index &lt; 5:\n    tf.compat.v1.logging.info(&quot;*** Example ***&quot;)\n    tf.compat.v1.logging.info(&quot;guid: %s&quot; % (example.guid))\n    tf.compat.v1.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.compat.v1.logging.info(&quot;input_ids: %s&quot; % &quot; &quot;.join([str(x) for x in input_ids]))\n    tf.compat.v1.logging.info(&quot;input_mask: %s&quot; % &quot; &quot;.join([str(x) for x in input_mask]))\n    tf.compat.v1.logging.info(&quot;segment_ids: %s&quot; % &quot; &quot;.join([str(x) for x in segment_ids]))\n    tf.compat.v1.logging.info(&quot;label: %s (id = %d)&quot; % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id,\n      is_real_example=True)\n  return feature\n\n\ndef file_based_convert_examples_to_features(\n    examples, label_list, max_seq_length, tokenizer, output_file):\n  &quot;&quot;&quot;Convert a set of `InputExample`s to a TFRecord file.&quot;&quot;&quot;\n\n  writer = tf.io.TFRecordWriter(output_file)\n\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.compat.v1.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    def create_int_feature(values):\n      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n      return f\n\n    features = collections.OrderedDict()\n    features[&quot;input_ids&quot;] = create_int_feature(feature.input_ids)\n    features[&quot;input_mask&quot;] = create_int_feature(feature.input_mask)\n    features[&quot;segment_ids&quot;] = create_int_feature(feature.segment_ids)\n    features[&quot;label_ids&quot;] = create_int_feature([feature.label_id])\n    features[&quot;is_real_example&quot;] = create_int_feature(\n        [int(feature.is_real_example)])\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    writer.write(tf_example.SerializeToString())\n  writer.close()\n\n\ndef file_based_input_fn_builder(input_file, seq_length, is_training,\n                                drop_remainder):\n  &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot;\n\n  name_to_features = &#123;\n      &quot;input_ids&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;input_mask&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;segment_ids&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;label_ids&quot;: tf.io.FixedLenFeature([], tf.int64),\n      &quot;is_real_example&quot;: tf.io.FixedLenFeature([], tf.int64),\n  &#125;\n\n  def _decode_record(record, name_to_features):\n    &quot;&quot;&quot;Decodes a record to a TensorFlow example.&quot;&quot;&quot;\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.compat.v1.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    &quot;&quot;&quot;The actual input function.&quot;&quot;&quot;\n    batch_size = params[&quot;batch_size&quot;]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn&#39;t matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.data.experimental.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n\n    return d\n\n  return input_fn\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  &quot;&quot;&quot;Truncates a sequence pair in place to the maximum length.&quot;&quot;&quot;\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that&#39;s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length &lt;= max_length:\n      break\n    if len(tokens_a) &gt; len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  &quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output()\n\n  hidden_size = output_layer.shape[-1]\n\n  output_weights = tf.compat.v1.get_variable(\n      &quot;output_weights&quot;, [num_labels, hidden_size],\n      initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.compat.v1.get_variable(\n      &quot;output_bias&quot;, [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.compat.v1.variable_scope(&quot;loss&quot;):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.compat.v1.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  &quot;&quot;&quot;Returns `model_fn` closure for TPUEstimator.&quot;&quot;&quot;\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    &quot;&quot;&quot;The `model_fn` for TPUEstimator.&quot;&quot;&quot;\n\n    tf.compat.v1.logging.info(&quot;*** Features ***&quot;)\n    for name in sorted(features.keys()):\n      tf.compat.v1.logging.info(&quot;  name = %s, shape = %s&quot; % (name, features[name].shape))\n\n    input_ids = features[&quot;input_ids&quot;]\n    input_mask = features[&quot;input_mask&quot;]\n    segment_ids = features[&quot;segment_ids&quot;]\n    label_ids = features[&quot;label_ids&quot;]\n    is_real_example = None\n    if &quot;is_real_example&quot; in features:\n      is_real_example = tf.cast(features[&quot;is_real_example&quot;], dtype=tf.float32)\n    else:\n      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    (total_loss, per_example_loss, logits, probabilities) = create_model(\n        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n        num_labels, use_one_hot_embeddings)\n\n    tvars = tf.compat.v1.trainable_variables()\n    initialized_variable_names = &#123;&#125;\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.compat.v1.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.compat.v1.logging.info(&quot;**** Trainable Variables ****&quot;)\n    for var in tvars:\n      init_string = &quot;&quot;\n      if var.name in initialized_variable_names:\n        init_string = &quot;, *INIT_FROM_CKPT*&quot;\n      tf.compat.v1.logging.info(&quot;  name = %s, shape = %s%s&quot;, var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        accuracy = tf.compat.v1.metrics.accuracy(\n            labels=label_ids, predictions=predictions, weights=is_real_example)\n        loss = tf.compat.v1.metrics.mean(values=per_example_loss, weights=is_real_example)\n        return &#123;\n            &quot;eval_accuracy&quot;: accuracy,\n            &quot;eval_loss&quot;: loss,\n        &#125;\n\n      eval_metrics = (metric_fn,\n                      [per_example_loss, label_ids, logits, is_real_example])\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions=&#123;&quot;probabilities&quot;: probabilities&#125;,\n          scaffold_fn=scaffold_fn)\n    return output_spec\n\n  return model_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot;\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    &quot;&quot;&quot;The actual input function.&quot;&quot;&quot;\n    batch_size = params[&quot;batch_size&quot;]\n\n    num_examples = len(features)\n\n    # This is for demo purposes and does NOT scale to large data sets. We do\n    # not use Dataset.from_generator() because that uses tf.py_func which is\n    # not TPU compatible. The right way to load data is with TFRecordReader.\n    d = tf.data.Dataset.from_tensor_slices(&#123;\n        &quot;input_ids&quot;:\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;input_mask&quot;:\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;segment_ids&quot;:\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;label_ids&quot;:\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    &#125;)\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer):\n  &quot;&quot;&quot;Convert a set of `InputExample`s to a list of `InputFeatures`.&quot;&quot;&quot;\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.compat.v1.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n\ndef serving_input_fn():\n    # 保存模型为SaveModel格式\n    # 采用最原始的feature方式，输入是feature Tensors。\n    # 如果采用build_parsing_serving_input_receiver_fn，则输入是tf.Examples\n\n    label_ids = tf.compat.v1.placeholder(tf.int32, [None, 3], name=&#39;label_ids&#39;)\n    input_ids = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;input_ids&#39;)\n    input_mask = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;input_mask&#39;)\n    segment_ids = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;segment_ids&#39;)\n    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(&#123;\n        &#39;label_ids&#39;: label_ids,\n        &#39;input_ids&#39;: input_ids,\n        &#39;input_mask&#39;: input_mask,\n        &#39;segment_ids&#39;: segment_ids,\n    &#125;)()\n    return input_fn\n\ndef main(_):\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n\n  processors = &#123;\n      &quot;cola&quot;: ColaProcessor,\n      &quot;mnli&quot;: MnliProcessor,\n      &quot;mrpc&quot;: MrpcProcessor,\n      &quot;xnli&quot;: XnliProcessor,\n  &#125;\n\n  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n                                                FLAGS.init_checkpoint)\n\n  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n    raise ValueError(\n        &quot;At least one of `do_train`, `do_eval` or `do_predict&#39; must be True.&quot;)\n\n  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n  if FLAGS.max_seq_length &gt; bert_config.max_position_embeddings:\n    raise ValueError(\n        &quot;Cannot use sequence length %d because the BERT model &quot;\n        &quot;was only trained up to sequence length %d&quot; %\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf.io.gfile.makedirs(FLAGS.trans_model_dir)\n\n  task_name = FLAGS.task_name.lower()\n\n  if task_name not in processors:\n    raise ValueError(&quot;Task not found: %s&quot; % (task_name))\n\n  processor = processors[task_name]()\n\n  label_list = processor.get_labels()\n\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:\n    train_examples = processor.get_train_examples(FLAGS.data_dir)\n    num_train_steps = int(\n        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n  model_fn = model_fn_builder(\n      bert_config=bert_config,\n      num_labels=len(label_list),\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size)\n\n  if FLAGS.do_train:\n    train_file = os.path.join(FLAGS.output_dir, &quot;train.tf_record&quot;)\n    file_based_convert_examples_to_features(\n        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n    tf.compat.v1.logging.info(&quot;***** Running training *****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d&quot;, len(train_examples))\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.train_batch_size)\n    tf.compat.v1.logging.info(&quot;  Num steps = %d&quot;, num_train_steps)\n    train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\n  if FLAGS.do_eval:\n    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n    num_actual_eval_examples = len(eval_examples)\n    if FLAGS.use_tpu:\n      # TPU requires a fixed batch size for all batches, therefore the number\n      # of examples must be a multiple of the batch size, or else examples\n      # will get dropped. So we pad with fake examples which are ignored\n      # later on. These do NOT count towards the metric (all tf.metrics\n      # support a per-instance weight, and these get a weight of 0.0).\n      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n        eval_examples.append(PaddingInputExample())\n\n    eval_file = os.path.join(FLAGS.output_dir, &quot;eval.tf_record&quot;)\n    file_based_convert_examples_to_features(\n        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n\n    tf.compat.v1.logging.info(&quot;***** Running evaluation *****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d (%d actual, %d padding)&quot;,\n                    len(eval_examples), num_actual_eval_examples,\n                    len(eval_examples) - num_actual_eval_examples)\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.eval_batch_size)\n\n    # This tells the estimator to run through the entire set.\n    eval_steps = None\n    # However, if running eval on the TPU, you will need to specify the\n    # number of steps.\n    if FLAGS.use_tpu:\n      assert len(eval_examples) % FLAGS.eval_batch_size == 0\n      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n\n    eval_drop_remainder = True if FLAGS.use_tpu else False\n    eval_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=eval_drop_remainder)\n\n    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n\n    # trans_model_dir模型转换后输出目录，将模型转换为saved model\n    estimator._export_to_tpu = False\n    estimator.export_savedmodel(FLAGS.trans_model_dir, serving_input_fn)\n\n    output_eval_file = os.path.join(FLAGS.output_dir, &quot;eval_results.txt&quot;)\n    with tf.io.gfile.GFile(output_eval_file, &quot;w&quot;) as writer:\n      tf.compat.v1.logging.info(&quot;***** Eval results *****&quot;)\n      for key in sorted(result.keys()):\n        tf.compat.v1.logging.info(&quot;  %s = %s&quot;, key, str(result[key]))\n        writer.write(&quot;%s = %s\\n&quot; % (key, str(result[key])))\n\n  if FLAGS.do_predict:\n    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n    num_actual_predict_examples = len(predict_examples)\n    if FLAGS.use_tpu:\n      # TPU requires a fixed batch size for all batches, therefore the number\n      # of examples must be a multiple of the batch size, or else examples\n      # will get dropped. So we pad with fake examples which are ignored\n      # later on.\n      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n        predict_examples.append(PaddingInputExample())\n\n    predict_file = os.path.join(FLAGS.output_dir, &quot;predict.tf_record&quot;)\n    file_based_convert_examples_to_features(predict_examples, label_list,\n                                            FLAGS.max_seq_length, tokenizer,\n                                            predict_file)\n\n    tf.compat.v1.logging.info(&quot;***** Running prediction*****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d (%d actual, %d padding)&quot;,\n                    len(predict_examples), num_actual_predict_examples,\n                    len(predict_examples) - num_actual_predict_examples)\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.predict_batch_size)\n\n    predict_drop_remainder = True if FLAGS.use_tpu else False\n    predict_input_fn = file_based_input_fn_builder(\n        input_file=predict_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=predict_drop_remainder)\n\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    output_predict_file = os.path.join(FLAGS.output_dir, &quot;test_results.tsv&quot;)\n    with tf.io.gfile.GFile(output_predict_file, &quot;w&quot;) as writer:\n      num_written_lines = 0\n      tf.compat.v1.logging.info(&quot;***** Predict results *****&quot;)\n      for (i, prediction) in enumerate(result):\n        probabilities = prediction[&quot;probabilities&quot;]\n        if i &gt;= num_actual_predict_examples:\n          break\n        output_line = &quot;\\t&quot;.join(\n            str(class_probability)\n            for class_probability in probabilities) + &quot;\\n&quot;\n        writer.write(output_line)\n        num_written_lines += 1\n    assert num_written_lines == num_actual_predict_examples\n\n\nif __name__ == &quot;__main__&quot;:\n  flags.mark_flag_as_required(&quot;data_dir&quot;)\n  flags.mark_flag_as_required(&quot;task_name&quot;)\n  flags.mark_flag_as_required(&quot;vocab_file&quot;)\n  flags.mark_flag_as_required(&quot;bert_config_file&quot;)\n  flags.mark_flag_as_required(&quot;output_dir&quot;)\n  flags.mark_flag_as_required(&quot;trans_model_dir&quot;)\n  app.run(main)</code></pre>\n<h5 id=\"optimization.py\">optimization.py</h5>\n<pre class=\"python3\"><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n  &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot;\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n\n  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n  # Implements linear decay of the learning rate.\n  learning_rate = tf.compat.v1.train.polynomial_decay(\n      learning_rate,\n      global_step,\n      num_train_steps,\n      end_learning_rate=0.0,\n      power=1.0,\n      cycle=False)\n\n  # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the\n  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n  if num_warmup_steps:\n    global_steps_int = tf.cast(global_step, tf.int32)\n    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n    global_steps_float = tf.cast(global_steps_int, tf.float32)\n    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n    warmup_percent_done = global_steps_float / warmup_steps_float\n    warmup_learning_rate = init_lr * warmup_percent_done\n\n    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)\n    learning_rate = (\n        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n  # It is recommended that you use this optimizer for fine tuning, since this\n  # is how the model was trained (note that the Adam m/v variables are NOT\n  # loaded from init_checkpoint.)\n  optimizer = AdamWeightDecayOptimizer(\n      learning_rate=learning_rate,\n      weight_decay_rate=0.01,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])\n\n  if use_tpu:\n    optimizer = tf.compat.v1.estimator.tpu.CrossShardOptimizer(optimizer)\n\n  tvars = tf.compat.v1.trainable_variables()\n  grads = tf.gradients(loss, tvars)\n\n  # This is how the model was pre-trained.\n  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n\n  train_op = optimizer.apply_gradients(\n      zip(grads, tvars), global_step=global_step)\n\n  # Normally the global step update is done inside of `apply_gradients`.\n  # However, `AdamWeightDecayOptimizer` doesn&#39;t do this. But if you use\n  # a different optimizer, you should probably take this line out.\n  new_global_step = global_step + 1\n  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n  return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.keras.optimizers.Optimizer):\n  &quot;&quot;&quot;A basic Adam optimizer that includes &quot;correct&quot; L2 weight decay.&quot;&quot;&quot;\n\n  def __init__(self,\n               learning_rate,\n               weight_decay_rate=0.0,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-6,\n               exclude_from_weight_decay=None,\n               name=&quot;AdamWeightDecayOptimizer&quot;):\n    &quot;&quot;&quot;Constructs a AdamWeightDecayOptimizer.&quot;&quot;&quot;\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    assignments = []\n    for (grad, param) in grads_and_vars:\n      if grad is None or param is None:\n        continue\n\n      param_name = self._get_variable_name(param.name)\n\n      m = tf.compat.v1.get_variable(\n          name=param_name + &quot;/adam_m&quot;,\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n      v = tf.compat.v1.get_variable(\n          name=param_name + &quot;/adam_v&quot;,\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n\n      # Standard Adam update.\n      next_m = (\n          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n      next_v = (\n          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                    tf.square(grad)))\n\n      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n      # Just adding the square of the weights to the loss function is *not*\n      # the correct way of using L2 regularization/weight decay with Adam,\n      # since that will interact with the m and v parameters in strange ways.\n      #\n      # Instead we want ot decay the weights in a manner that doesn&#39;t interact\n      # with the m/v parameters. This is equivalent to adding the square\n      # of the weights to the loss with plain (non-momentum) SGD.\n      if self._do_use_weight_decay(param_name):\n        update += self.weight_decay_rate * param\n\n      update_with_lr = self.learning_rate * update\n\n      next_param = param - update_with_lr\n\n      assignments.extend(\n          [param.assign(next_param),\n           m.assign(next_m),\n           v.assign(next_v)])\n    return tf.group(*assignments, name=name)\n\n  def _do_use_weight_decay(self, param_name):\n    &quot;&quot;&quot;Whether to use L2 weight decay for `param_name`.&quot;&quot;&quot;\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n\n  def _get_variable_name(self, param_name):\n    &quot;&quot;&quot;Get the variable name from the tensor name.&quot;&quot;&quot;\n    m = re.match(&quot;^(.*):\\\\d+$&quot;, param_name)\n    if m is not None:\n      param_name = m.group(1)\n    return param_name\n</code></pre>\n<h5 id=\"tokenization.py\">tokenization.py</h5>\n<pre class=\"python3\"><code>\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n  &quot;&quot;&quot;Checks whether the casing config is consistent with the checkpoint name.&quot;&quot;&quot;\n\n  # The casing has to be passed in by the user and there is no explicit check\n  # as to whether it matches the checkpoint. The casing information probably\n  # should have been stored in the bert_config.json file, but it&#39;s not, so\n  # we have to heuristically detect it to validate.\n\n  if not init_checkpoint:\n    return\n\n  m = re.match(&quot;^.*?([A-Za-z0-9_-]+)/bert_model.ckpt&quot;, init_checkpoint)\n  if m is None:\n    return\n\n  model_name = m.group(1)\n\n  lower_models = [\n      &quot;uncased_L-24_H-1024_A-16&quot;, &quot;uncased_L-12_H-768_A-12&quot;,\n      &quot;multilingual_L-12_H-768_A-12&quot;, &quot;chinese_L-12_H-768_A-12&quot;\n  ]\n\n  cased_models = [\n      &quot;cased_L-12_H-768_A-12&quot;, &quot;cased_L-24_H-1024_A-16&quot;,\n      &quot;multi_cased_L-12_H-768_A-12&quot;\n  ]\n\n  is_bad_config = False\n  if model_name in lower_models and not do_lower_case:\n    is_bad_config = True\n    actual_flag = &quot;False&quot;\n    case_name = &quot;lowercased&quot;\n    opposite_flag = &quot;True&quot;\n\n  if model_name in cased_models and do_lower_case:\n    is_bad_config = True\n    actual_flag = &quot;True&quot;\n    case_name = &quot;cased&quot;\n    opposite_flag = &quot;False&quot;\n\n  if is_bad_config:\n    raise ValueError(\n        &quot;You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. &quot;\n        &quot;However, `%s` seems to be a %s model, so you &quot;\n        &quot;should pass in `--do_lower_case=%s` so that the fine-tuning matches &quot;\n        &quot;how the model was pre-training. If this error is wrong, please &quot;\n        &quot;just comment out this check.&quot; % (actual_flag, init_checkpoint,\n                                          model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n  &quot;&quot;&quot;Converts `text` to Unicode (if it&#39;s not already), assuming utf-8 input.&quot;&quot;&quot;\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  else:\n    raise ValueError(&quot;Not running on Python2 or Python 3?&quot;)\n\n\ndef printable_text(text):\n  &quot;&quot;&quot;Returns text encoded in a way suitable for print or `tf.logging`.&quot;&quot;&quot;\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it&#39;s a Unicode string and in the other it&#39;s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(&quot;utf-8&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  else:\n    raise ValueError(&quot;Not running on Python2 or Python 3?&quot;)\n\n\ndef load_vocab(vocab_file):\n  &quot;&quot;&quot;Loads a vocabulary file into a dictionary.&quot;&quot;&quot;\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  &quot;&quot;&quot;Converts a sequence of [tokens|ids] using the vocab.&quot;&quot;&quot;\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  &quot;&quot;&quot;Runs basic whitespace cleaning and splitting on a piece of text.&quot;&quot;&quot;\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  &quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125;\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  &quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;\n\n  def __init__(self, do_lower_case=True):\n    &quot;&quot;&quot;Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    &quot;&quot;&quot;\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    &quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn&#39;t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don&#39;t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    &quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;\n    text = unicodedata.normalize(&quot;NFD&quot;, text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == &quot;Mn&quot;:\n        continue\n      output.append(char)\n    return &quot;&quot;.join(output)\n\n  def _run_split_on_punc(self, text):\n    &quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i &lt; len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return [&quot;&quot;.join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    &quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append(&quot; &quot;)\n        output.append(char)\n        output.append(&quot; &quot;)\n      else:\n        output.append(char)\n    return &quot;&quot;.join(output)\n\n  def _is_chinese_char(self, cp):\n    &quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;\n    # This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or  #\n        (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or  #\n        (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or  #\n        (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or  #\n        (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or  #\n        (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or\n        (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or  #\n        (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    &quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append(&quot; &quot;)\n      else:\n        output.append(char)\n    return &quot;&quot;.join(output)\n\n\nclass WordpieceTokenizer(object):\n  &quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;\n\n  def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    &quot;&quot;&quot;Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = &quot;unaffable&quot;\n      output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    &quot;&quot;&quot;\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) &gt; self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start &lt; len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start &lt; end:\n          substr = &quot;&quot;.join(chars[start:end])\n          if start &gt; 0:\n            substr = &quot;##&quot; + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  &quot;&quot;&quot;Checks whether `chars` is a whitespace character.&quot;&quot;&quot;\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == &quot; &quot; or char == &quot;\\t&quot; or char == &quot;\\n&quot; or char == &quot;\\r&quot;:\n    return True\n  cat = unicodedata.category(char)\n  if cat == &quot;Zs&quot;:\n    return True\n  return False\n\n\ndef _is_control(char):\n  &quot;&quot;&quot;Checks whether `chars` is a control character.&quot;&quot;&quot;\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == &quot;\\t&quot; or char == &quot;\\n&quot; or char == &quot;\\r&quot;:\n    return False\n  cat = unicodedata.category(char)\n  if cat in (&quot;Cc&quot;, &quot;Cf&quot;):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  &quot;&quot;&quot;Checks whether `chars` is a punctuation character.&quot;&quot;&quot;\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as &quot;^&quot;, &quot;$&quot;, and &quot;`&quot; are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp &gt;= 33 and cp &lt;= 47) or (cp &gt;= 58 and cp &lt;= 64) or\n      (cp &gt;= 91 and cp &lt;= 96) or (cp &gt;= 123 and cp &lt;= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(&quot;P&quot;):\n    return True\n  return False\n</code></pre>\n<h5 id=\"modeling.py\">modeling.py</h5>\n<pre class=\"python3\"><code>\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  &quot;&quot;&quot;Configuration for `BertModel`.&quot;&quot;&quot;\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=&quot;gelu&quot;,\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    &quot;&quot;&quot;Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the &quot;intermediate&quot; (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    &quot;&quot;&quot;\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    &quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot;\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    &quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot;\n    with tf.io.gfile.GFile(json_file, &quot;r&quot;) as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    &quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot;\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    &quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot;\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\\n&quot;\n\n\nclass BertModel(object):\n  &quot;&quot;&quot;BERT model (&quot;Bidirectional Encoder Representations from Transformers&quot;).\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.compact.v1.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...</code></pre>\n<p>\"\"\"</p>\n<p>def <strong>init</strong>(self, config, is_training, input_ids,\ninput_mask=None, token_type_ids=None, use_one_hot_embeddings=False,\nscope=None): \"\"\"Constructor for BertModel.</p>\n<pre><code>Args:\n  config: `BertConfig` instance.\n  is_training: bool. true for training model, false for eval model. Controls\n    whether dropout will be applied.\n  input_ids: int32 Tensor of shape [batch_size, seq_length].\n  input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n  token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n  use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n    embeddings or tf.embedding_lookup() for the word embeddings.\n  scope: (optional) variable scope. Defaults to &quot;bert&quot;.\n\nRaises:\n  ValueError: The config is invalid or one of the input tensor shapes\n    is invalid.\n&quot;&quot;&quot;\nconfig = copy.deepcopy(config)\nif not is_training:\n  config.hidden_dropout_prob = 0.0\n  config.attention_probs_dropout_prob = 0.0\n\ninput_shape = get_shape_list(input_ids, expected_rank=2)\nbatch_size = input_shape[0]\nseq_length = input_shape[1]\n\nif input_mask is None:\n  input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\nif token_type_ids is None:\n  token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\nwith tf.compat.v1.variable_scope(scope, default_name=&quot;bert&quot;):\n  with tf.compat.v1.variable_scope(&quot;embeddings&quot;):\n    # Perform embedding lookup on the word ids.\n    (self.embedding_output, self.embedding_table) = embedding_lookup(\n        input_ids=input_ids,\n        vocab_size=config.vocab_size,\n        embedding_size=config.hidden_size,\n        initializer_range=config.initializer_range,\n        word_embedding_name=&quot;word_embeddings&quot;,\n        use_one_hot_embeddings=use_one_hot_embeddings)\n\n    # Add positional embeddings and token type embeddings, then layer\n    # normalize and perform dropout.\n    self.embedding_output = embedding_postprocessor(\n        input_tensor=self.embedding_output,\n        use_token_type=True,\n        token_type_ids=token_type_ids,\n        token_type_vocab_size=config.type_vocab_size,\n        token_type_embedding_name=&quot;token_type_embeddings&quot;,\n        use_position_embeddings=True,\n        position_embedding_name=&quot;position_embeddings&quot;,\n        initializer_range=config.initializer_range,\n        max_position_embeddings=config.max_position_embeddings,\n        dropout_prob=config.hidden_dropout_prob)\n\n  with tf.compat.v1.variable_scope(&quot;encoder&quot;):\n    # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n    # mask of shape [batch_size, seq_length, seq_length] which is used\n    # for the attention scores.\n    attention_mask = create_attention_mask_from_input_mask(\n        input_ids, input_mask)\n\n    # Run the stacked transformer.\n    # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n    self.all_encoder_layers = transformer_model(\n        input_tensor=self.embedding_output,\n        attention_mask=attention_mask,\n        hidden_size=config.hidden_size,\n        num_hidden_layers=config.num_hidden_layers,\n        num_attention_heads=config.num_attention_heads,\n        intermediate_size=config.intermediate_size,\n        intermediate_act_fn=get_activation(config.hidden_act),\n        hidden_dropout_prob=config.hidden_dropout_prob,\n        attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n        initializer_range=config.initializer_range,\n        do_return_all_layers=True)\n\n  self.sequence_output = self.all_encoder_layers[-1]\n  # The &quot;pooler&quot; converts the encoded sequence tensor of shape\n  # [batch_size, seq_length, hidden_size] to a tensor of shape\n  # [batch_size, hidden_size]. This is necessary for segment-level\n  # (or segment-pair-level) classification tasks where we need a fixed\n  # dimensional representation of the segment.\n  with tf.compat.v1.variable_scope(&quot;pooler&quot;):\n    # We &quot;pool&quot; the model by simply taking the hidden state corresponding\n    # to the first token. We assume that this has been pre-trained\n    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n    self.pooled_output = tf.compat.v1.layers.dense(\n        first_token_tensor,\n        config.hidden_size,\n        activation=tf.tanh,\n        kernel_initializer=create_initializer(config.initializer_range))</code></pre>\n<p>def get_pooled_output(self): return self.pooled_output</p>\n<p>def get_sequence_output(self): \"\"\"Gets final hidden layer of\nencoder.</p>\n<pre><code>Returns:\n  float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n  to the final hidden of the transformer encoder.\n&quot;&quot;&quot;\nreturn self.sequence_output</code></pre>\n<p>def get_all_encoder_layers(self): return self.all_encoder_layers</p>\n<p>def get_embedding_output(self): \"\"\"Gets output of the embedding\nlookup (i.e., input to the transformer).</p>\n<pre><code>Returns:\n  float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n  to the output of the embedding layer, after summing the word\n  embeddings with the positional embeddings and the token type embeddings,\n  then performing layer normalization. This is the input to the transformer.\n&quot;&quot;&quot;\nreturn self.embedding_output</code></pre>\n<p>def get_embedding_table(self): return self.embedding_table</p>\n<p>def gelu(x): \"\"\"Gaussian Error Linear Unit.</p>\n<p>This is a smoother version of the RELU. Original paper:\nhttps://arxiv.org/abs/1606.08415 Args: x: float Tensor to perform\nactivation.</p>\n<p>Returns: <code>x</code> with the GELU activation applied. \"\"\" cdf =\n0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x,\n3))))) return x * cdf</p>\n<p>def get_activation(activation_string): \"\"\"Maps a string to a Python\nfunction, e.g., \"relu\" =&gt; <code>tf.nn.relu</code>.</p>\n<p>Args: activation_string: String name of the activation function.</p>\n<p>Returns: A Python function corresponding to the activation function.\nIf <code>activation_string</code> is None, empty, or \"linear\", this will\nreturn None. If <code>activation_string</code> is not a string, it will\nreturn <code>activation_string</code>.</p>\n<p>Raises: ValueError: The <code>activation_string</code> does not\ncorrespond to a known activation. \"\"\"</p>\n<p># We assume that anything that\"s not a string is already an\nactivation # function, so we just return it. if not\nisinstance(activation_string, six.string_types): return\nactivation_string</p>\n<p>if not activation_string: return None</p>\n<p>act = activation_string.lower() if act == \"linear\": return None elif\nact == \"relu\": return tf.nn.relu elif act == \"gelu\": return gelu elif\nact == \"tanh\": return tf.tanh else: raise ValueError(\"Unsupported\nactivation: %s\" % act)</p>\n<p>def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n\"\"\"Compute the union of the current variables and checkpoint\nvariables.\"\"\" assignment_map = {} initialized_variable_names = {}</p>\n<p>name_to_variable = collections.OrderedDict() for var in tvars: name =\nvar.name m = re.match(\"^(.*):\\d+$\", name) if m is not None: name =\nm.group(1) name_to_variable[name] = var</p>\n<p>init_vars = tf.compat.v1.train.list_variables(init_checkpoint)</p>\n<p>assignment_map = collections.OrderedDict() for x in init_vars: (name,\nvar) = (x[0], x[1]) if name not in name_to_variable: continue\nassignment_map[name] = name initialized_variable_names[name] = 1\ninitialized_variable_names[name + \":0\"] = 1</p>\n<p>return (assignment_map, initialized_variable_names)</p>\n<p>def dropout(input_tensor, dropout_prob): \"\"\"Perform dropout.</p>\n<p>Args: input_tensor: float Tensor. dropout_prob: Python float. The\nprobability of dropping out a value (NOT of <em>keeping</em> a dimension\nas in <code>tf.nn.dropout</code>).</p>\n<p>Returns: A version of <code>input_tensor</code> with dropout applied.\n\"\"\" if dropout_prob is None or dropout_prob == 0.0: return\ninput_tensor</p>\n<p>output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob) return\noutput</p>\n<p>def layer_norm(input_tensor, name=None): \"\"\"Run layer normalization\non the last dimension of the tensor.\"\"\" layernorm =\ntf.keras.layers.LayerNormalization(axis=-1) return\nlayernorm(input_tensor)</p>\n<p>def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n\"\"\"Runs layer normalization followed by dropout.\"\"\" output_tensor =\nlayer_norm(input_tensor, name) output_tensor = dropout(output_tensor,\ndropout_prob) return output_tensor</p>\n<p>def create_initializer(initializer_range=0.02): \"\"\"Creates a\n<code>truncated_normal_initializer</code> with the given range.\"\"\"\nreturn\ntf.compat.v1.truncated_normal_initializer(stddev=initializer_range)</p>\n<p>def embedding_lookup(input_ids, vocab_size, embedding_size=128,\ninitializer_range=0.02, word_embedding_name=\"word_embeddings\",\nuse_one_hot_embeddings=False): \"\"\"Looks up words embeddings for id\ntensor.</p>\n<p>Args: input_ids: int32 Tensor of shape [batch_size, seq_length]\ncontaining word ids. vocab_size: int. Size of the embedding vocabulary.\nembedding_size: int. Width of the word embeddings. initializer_range:\nfloat. Embedding initialization range. word_embedding_name: string. Name\nof the embedding table. use_one_hot_embeddings: bool. If True, use\none-hot method for word embeddings. If False, use\n<code>tf.gather()</code>.</p>\n<p>Returns: float Tensor of shape [batch_size, seq_length,\nembedding_size]. \"\"\" # This function assumes that the input is of shape\n[batch_size, seq_length, # num_inputs]. # # If the input is a 2D tensor\nof shape [batch_size, seq_length], we # reshape to [batch_size,\nseq_length, 1]. if input_ids.shape.ndims == 2: input_ids =\ntf.expand_dims(input_ids, axis=[-1])</p>\n<p>embedding_table = tf.compat.v1.get_variable(\nname=word_embedding_name, shape=[vocab_size, embedding_size],\ninitializer=create_initializer(initializer_range))</p>\n<p>flat_input_ids = tf.reshape(input_ids, [-1]) if\nuse_one_hot_embeddings: one_hot_input_ids = tf.one_hot(flat_input_ids,\ndepth=vocab_size) output = tf.matmul(one_hot_input_ids, embedding_table)\nelse: output = tf.gather(embedding_table, flat_input_ids)</p>\n<p>input_shape = get_shape_list(input_ids)</p>\n<p>output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] *\nembedding_size]) return (output, embedding_table)</p>\n<p>def embedding_postprocessor(input_tensor, use_token_type=False,\ntoken_type_ids=None, token_type_vocab_size=16,\ntoken_type_embedding_name=\"token_type_embeddings\",\nuse_position_embeddings=True,\nposition_embedding_name=\"position_embeddings\", initializer_range=0.02,\nmax_position_embeddings=512, dropout_prob=0.1): \"\"\"Performs various\npost-processing on a word embedding tensor.</p>\n<p>Args: input_tensor: float Tensor of shape [batch_size, seq_length,\nembedding_size]. use_token_type: bool. Whether to add embeddings for\n<code>token_type_ids</code>. token_type_ids: (optional) int32 Tensor of\nshape [batch_size, seq_length]. Must be specified if\n<code>use_token_type</code> is True. token_type_vocab_size: int. The\nvocabulary size of <code>token_type_ids</code>.\ntoken_type_embedding_name: string. The name of the embedding table\nvariable for token type ids. use_position_embeddings: bool. Whether to\nadd position embeddings for the position of each token in the sequence.\nposition_embedding_name: string. The name of the embedding table\nvariable for positional embeddings. initializer_range: float. Range of\nthe weight initialization. max_position_embeddings: int. Maximum\nsequence length that might ever be used with this model. This can be\nlonger than the sequence length of input_tensor, but cannot be shorter.\ndropout_prob: float. Dropout probability applied to the final output\ntensor.</p>\n<p>Returns: float tensor with same shape as\n<code>input_tensor</code>.</p>\n<p>Raises: ValueError: One of the tensor shapes or input values is\ninvalid. \"\"\" input_shape = get_shape_list(input_tensor, expected_rank=3)\nbatch_size = input_shape[0] seq_length = input_shape[1] width =\ninput_shape[2]</p>\n<p>output = input_tensor</p>\n<p>if use_token_type: if token_type_ids is None: raise\nValueError(\"<code>token_type_ids</code> must be specified if\"\n\"<code>use_token_type</code> is True.\") token_type_table =\ntf.compat.v1.get_variable( name=token_type_embedding_name,\nshape=[token_type_vocab_size, width],\ninitializer=create_initializer(initializer_range)) # This vocab will be\nsmall so we always do one-hot here, since it is always # faster for a\nsmall vocabulary. flat_token_type_ids = tf.reshape(token_type_ids, [-1])\none_hot_ids = tf.one_hot(flat_token_type_ids,\ndepth=token_type_vocab_size) token_type_embeddings =\ntf.matmul(one_hot_ids, token_type_table) token_type_embeddings =\ntf.reshape(token_type_embeddings, [batch_size, seq_length, width])\noutput += token_type_embeddings</p>\n<p>if use_position_embeddings: assert_op =\ntf.debugging.assert_less_equal(seq_length, max_position_embeddings) with\ntf.control_dependencies([assert_op]): full_position_embeddings =\ntf.compat.v1.get_variable( name=position_embedding_name,\nshape=[max_position_embeddings, width],\ninitializer=create_initializer(initializer_range)) # Since the position\nembedding table is a learned variable, we create it # using a (long)\nsequence length <code>max_position_embeddings</code>. The actual #\nsequence length might be shorter than this, for faster training of #\ntasks that do not have long sequences. # # So\n<code>full_position_embeddings</code> is effectively an embedding table\n# for position [0, 1, 2, ..., max_position_embeddings-1], and the\ncurrent # sequence has positions [0, 1, 2, ... seq_length-1], so we can\njust # perform a slice. position_embeddings =\ntf.slice(full_position_embeddings, [0, 0], [seq_length, -1]) num_dims =\nlen(output.shape.as_list())</p>\n<pre><code>  # Only the last two dimensions are relevant (`seq_length` and `width`), so\n  # we broadcast among the first dimensions, which is typically just\n  # the batch size.\n  position_broadcast_shape = []\n  for _ in range(num_dims - 2):\n    position_broadcast_shape.append(1)\n  position_broadcast_shape.extend([seq_length, width])\n  position_embeddings = tf.reshape(position_embeddings,\n                                   position_broadcast_shape)\n  output += position_embeddings</code></pre>\n<p>output = layer_norm_and_dropout(output, dropout_prob) return\noutput</p>\n<p>def create_attention_mask_from_input_mask(from_tensor, to_mask):\n\"\"\"Create 3D attention mask from a 2D tensor mask.</p>\n<p>Args: from_tensor: 2D or 3D Tensor of shape [batch_size,\nfrom_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size,\nto_seq_length].</p>\n<p>Returns: float Tensor of shape [batch_size, from_seq_length,\nto_seq_length]. \"\"\" from_shape = get_shape_list(from_tensor,\nexpected_rank=[2, 3]) batch_size = from_shape[0] from_seq_length =\nfrom_shape[1]</p>\n<p>to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length =\nto_shape[1]</p>\n<p>to_mask = tf.cast( tf.reshape(to_mask, [batch_size, 1,\nto_seq_length]), tf.float32)</p>\n<p># We don't assume that <code>from_tensor</code> is a mask (although\nit could be). We # don't actually care if we attend <em>from</em>\npadding tokens (only <em>to</em> padding) # tokens so we create a tensor\nof all ones. # # <code>broadcast_ones</code> = [batch_size,\nfrom_seq_length, 1] broadcast_ones = tf.ones( shape=[batch_size,\nfrom_seq_length, 1], dtype=tf.float32)</p>\n<p># Here we broadcast along two dimensions to create the mask. mask =\nbroadcast_ones * to_mask</p>\n<p>return mask</p>\n<p>def attention_layer(from_tensor, to_tensor, attention_mask=None,\nnum_attention_heads=1, size_per_head=512, query_act=None, key_act=None,\nvalue_act=None, attention_probs_dropout_prob=0.0,\ninitializer_range=0.02, do_return_2d_tensor=False, batch_size=None,\nfrom_seq_length=None, to_seq_length=None): \"\"\"Performs multi-headed\nattention from <code>from_tensor</code> to <code>to_tensor</code>.</p>\n<p>This is an implementation of multi-headed attention based on\n\"Attention is all you Need\". If <code>from_tensor</code> and\n<code>to_tensor</code> are the same, then this is self-attention. Each\ntimestep in <code>from_tensor</code> attends to the corresponding\nsequence in <code>to_tensor</code>, and returns a fixed-with vector.</p>\n<p>This function first projects <code>from_tensor</code> into a \"query\"\ntensor and <code>to_tensor</code> into \"key\" and \"value\" tensors. These\nare (effectively) a list of tensors of length\n<code>num_attention_heads</code>, where each tensor is of shape\n[batch_size, seq_length, size_per_head].</p>\n<p>Then, the query and key tensors are dot-producted and scaled. These\nare softmaxed to obtain attention probabilities. The value tensors are\nthen interpolated by these probabilities, then concatenated back to a\nsingle tensor and returned.</p>\n<p>In practice, the multi-headed attention are done with transposes and\nreshapes rather than actual separate tensors.</p>\n<p>Args: from_tensor: float Tensor of shape [batch_size,\nfrom_seq_length, from_width]. to_tensor: float Tensor of shape\n[batch_size, to_seq_length, to_width]. attention_mask: (optional) int32\nTensor of shape [batch_size, from_seq_length, to_seq_length]. The values\nshould be 1 or 0. The attention scores will effectively be set to\n-infinity for any positions in the mask that are 0, and will be\nunchanged for positions that are 1. num_attention_heads: int. Number of\nattention heads. size_per_head: int. Size of each attention head.\nquery_act: (optional) Activation function for the query transform.\nkey_act: (optional) Activation function for the key transform.\nvalue_act: (optional) Activation function for the value transform.\nattention_probs_dropout_prob: (optional) float. Dropout probability of\nthe attention probabilities. initializer_range: float. Range of the\nweight initializer. do_return_2d_tensor: bool. If True, the output will\nbe of shape [batch_size * from_seq_length, num_attention_heads *\nsize_per_head]. If False, the output will be of shape [batch_size,\nfrom_seq_length, num_attention_heads * size_per_head]. batch_size:\n(Optional) int. If the input is 2D, this might be the batch size of the\n3D version of the <code>from_tensor</code> and <code>to_tensor</code>.\nfrom_seq_length: (Optional) If the input is 2D, this might be the seq\nlength of the 3D version of the <code>from_tensor</code>. to_seq_length:\n(Optional) If the input is 2D, this might be the seq length of the 3D\nversion of the <code>to_tensor</code>.</p>\n<p>Returns: float Tensor of shape [batch_size, from_seq_length,\nnum_attention_heads * size_per_head]. (If\n<code>do_return_2d_tensor</code> is true, this will be of shape\n[batch_size * from_seq_length, num_attention_heads *\nsize_per_head]).</p>\n<p>Raises: ValueError: Any of the arguments or tensor shapes are\ninvalid. \"\"\"</p>\n<p>def transpose_for_scores(input_tensor, batch_size,\nnum_attention_heads, seq_length, width): output_tensor = tf.reshape(\ninput_tensor, [batch_size, seq_length, num_attention_heads, width])</p>\n<pre><code>output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\nreturn output_tensor</code></pre>\n<p>from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\nto_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</p>\n<p>if len(from_shape) != len(to_shape): raise ValueError( \"The rank of\n<code>from_tensor</code> must match the rank of\n<code>to_tensor</code>.\")</p>\n<p>if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length =\nfrom_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if\n(batch_size is None or from_seq_length is None or to_seq_length is\nNone): raise ValueError( \"When passing in rank 2 tensors to\nattention_layer, the values \" \"for <code>batch_size</code>,\n<code>from_seq_length</code>, and <code>to_seq_length</code> \" \"must all\nbe specified.\")</p>\n<p># Scalar dimensions referenced here: # B = batch size (number of\nsequences) # F = <code>from_tensor</code> sequence length # T =\n<code>to_tensor</code> sequence length # N =\n<code>num_attention_heads</code> # H = <code>size_per_head</code></p>\n<p>from_tensor_2d = reshape_to_matrix(from_tensor) to_tensor_2d =\nreshape_to_matrix(to_tensor)</p>\n<p># <code>query_layer</code> = [B<em>F, N</em>H] query_layer =\ntf.compat.v1.layers.dense( from_tensor_2d, num_attention_heads *\nsize_per_head, activation=query_act, name=\"query\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>key_layer</code> = [B<em>T, N</em>H] key_layer =\ntf.compat.v1.layers.dense( to_tensor_2d, num_attention_heads *\nsize_per_head, activation=key_act, name=\"key\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>value_layer</code> = [B<em>T, N</em>H] value_layer =\ntf.compat.v1.layers.dense( to_tensor_2d, num_attention_heads *\nsize_per_head, activation=value_act, name=\"value\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>query_layer</code> = [B, N, F, H] query_layer =\ntranspose_for_scores(query_layer, batch_size, num_attention_heads,\nfrom_seq_length, size_per_head)</p>\n<p># <code>key_layer</code> = [B, N, T, H] key_layer =\ntranspose_for_scores(key_layer, batch_size, num_attention_heads,\nto_seq_length, size_per_head)</p>\n<p># Take the dot product between \"query\" and \"key\" to get the raw #\nattention scores. # <code>attention_scores</code> = [B, N, F, T]\nattention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\nattention_scores = tf.multiply(attention_scores, 1.0 /\nmath.sqrt(float(size_per_head)))</p>\n<p>if attention_mask is not None: # <code>attention_mask</code> = [B, 1,\nF, T] attention_mask = tf.expand_dims(attention_mask, axis=[1])</p>\n<pre><code># Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n# masked positions, this operation will create a tensor which is 0.0 for\n# positions we want to attend and -10000.0 for masked positions.\nadder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n# Since we are adding it to the raw scores before the softmax, this is\n# effectively the same as removing these entirely.\nattention_scores += adder</code></pre>\n<p># Normalize the attention scores to probabilities. #\n<code>attention_probs</code> = [B, N, F, T] attention_probs =\ntf.nn.softmax(attention_scores)</p>\n<p># This is actually dropping out entire tokens to attend to, which\nmight # seem a bit unusual, but is taken from the original Transformer\npaper. attention_probs = dropout(attention_probs,\nattention_probs_dropout_prob)</p>\n<p># <code>value_layer</code> = [B, T, N, H] value_layer = tf.reshape(\nvalue_layer, [batch_size, to_seq_length, num_attention_heads,\nsize_per_head])</p>\n<p># <code>value_layer</code> = [B, N, T, H] value_layer =\ntf.transpose(value_layer, [0, 2, 1, 3])</p>\n<p># <code>context_layer</code> = [B, N, F, H] context_layer =\ntf.matmul(attention_probs, value_layer)</p>\n<p># <code>context_layer</code> = [B, F, N, H] context_layer =\ntf.transpose(context_layer, [0, 2, 1, 3])</p>\n<p>if do_return_2d_tensor: # <code>context_layer</code> = [B<em>F,\nN</em>H] context_layer = tf.reshape( context_layer, [batch_size *\nfrom_seq_length, num_attention_heads * size_per_head]) else: #\n<code>context_layer</code> = [B, F, N*H] context_layer = tf.reshape(\ncontext_layer, [batch_size, from_seq_length, num_attention_heads *\nsize_per_head])</p>\n<p>return context_layer</p>\n<p>def transformer_model(input_tensor, attention_mask=None,\nhidden_size=768, num_hidden_layers=12, num_attention_heads=12,\nintermediate_size=3072, intermediate_act_fn=gelu,\nhidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1,\ninitializer_range=0.02, do_return_all_layers=False): \"\"\"Multi-headed,\nmulti-layer Transformer from \"Attention is All You Need\".</p>\n<p>This is almost an exact implementation of the original Transformer\nencoder.</p>\n<p>See the original paper: https://arxiv.org/abs/1706.03762</p>\n<p>Also see:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</p>\n<p>Args: input_tensor: float Tensor of shape [batch_size, seq_length,\nhidden_size]. attention_mask: (optional) int32 Tensor of shape\n[batch_size, seq_length, seq_length], with 1 for positions that can be\nattended to and 0 in positions that should not be. hidden_size: int.\nHidden size of the Transformer. num_hidden_layers: int. Number of layers\n(blocks) in the Transformer. num_attention_heads: int. Number of\nattention heads in the Transformer. intermediate_size: int. The size of\nthe \"intermediate\" (a.k.a., feed forward) layer. intermediate_act_fn:\nfunction. The non-linear activation function to apply to the output of\nthe intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout\nprobability for the hidden layers. attention_probs_dropout_prob: float.\nDropout probability of the attention probabilities. initializer_range:\nfloat. Range of the initializer (stddev of truncated normal).\ndo_return_all_layers: Whether to also return all layers or just the\nfinal layer.</p>\n<p>Returns: float Tensor of shape [batch_size, seq_length, hidden_size],\nthe final hidden layer of the Transformer.</p>\n<p>Raises: ValueError: A Tensor shape or parameter is invalid. \"\"\" if\nhidden_size % num_attention_heads != 0: raise ValueError( \"The hidden\nsize (%d) is not a multiple of the number of attention \" \"heads (%d)\" %\n(hidden_size, num_attention_heads))</p>\n<p>attention_head_size = int(hidden_size / num_attention_heads)\ninput_shape = get_shape_list(input_tensor, expected_rank=3) batch_size =\ninput_shape[0] seq_length = input_shape[1] input_width =\ninput_shape[2]</p>\n<p># The Transformer performs sum residuals on all layers so the input\nneeds # to be the same as the hidden size. if input_width !=\nhidden_size: raise ValueError(\"The width of the input tensor (%d) !=\nhidden size (%d)\" % (input_width, hidden_size))</p>\n<p># We keep the representation as a 2D tensor to avoid re-shaping it\nback and # forth from a 3D tensor to a 2D tensor. Re-shapes are normally\nfree on # the GPU/CPU but may not be free on the TPU, so we want to\nminimize them to # help the optimizer. prev_output =\nreshape_to_matrix(input_tensor)</p>\n<p>all_layer_outputs = [] for layer_idx in range(num_hidden_layers):\nwith tf.compat.v1.variable_scope(\"layer_%d\" % layer_idx): layer_input =\nprev_output</p>\n<pre><code>  with tf.compat.v1.variable_scope(&quot;attention&quot;):\n    attention_heads = []\n    with tf.compat.v1.variable_scope(&quot;self&quot;):\n      attention_head = attention_layer(\n          from_tensor=layer_input,\n          to_tensor=layer_input,\n          attention_mask=attention_mask,\n          num_attention_heads=num_attention_heads,\n          size_per_head=attention_head_size,\n          attention_probs_dropout_prob=attention_probs_dropout_prob,\n          initializer_range=initializer_range,\n          do_return_2d_tensor=True,\n          batch_size=batch_size,\n          from_seq_length=seq_length,\n          to_seq_length=seq_length)\n      attention_heads.append(attention_head)\n\n    attention_output = None\n    if len(attention_heads) == 1:\n      attention_output = attention_heads[0]\n    else:\n      # In the case where we have other sequences, we just concatenate\n      # them to the self-attention head before the projection.\n      attention_output = tf.concat(attention_heads, axis=-1)\n\n    # Run a linear projection of `hidden_size` then add a residual\n    # with `layer_input`.\n    with tf.compat.v1.variable_scope(&quot;output&quot;):\n      attention_output = tf.compat.v1.layers.dense(\n          attention_output,\n          hidden_size,\n          kernel_initializer=create_initializer(initializer_range))\n      attention_output = dropout(attention_output, hidden_dropout_prob)\n      attention_output = layer_norm(attention_output + layer_input)\n\n  # The activation is only applied to the &quot;intermediate&quot; hidden layer.\n  with tf.compat.v1.variable_scope(&quot;intermediate&quot;):\n    intermediate_output = tf.compat.v1.layers.dense(\n        attention_output,\n        intermediate_size,\n        activation=intermediate_act_fn,\n        kernel_initializer=create_initializer(initializer_range))\n\n  # Down-project back to `hidden_size` then add the residual.\n  with tf.compat.v1.variable_scope(&quot;output&quot;):\n    layer_output = tf.compat.v1.layers.dense(\n        intermediate_output,\n        hidden_size,\n        kernel_initializer=create_initializer(initializer_range))\n    layer_output = dropout(layer_output, hidden_dropout_prob)\n    layer_output = layer_norm(layer_output + attention_output)\n    prev_output = layer_output\n    all_layer_outputs.append(layer_output)</code></pre>\n<p>if do_return_all_layers: final_outputs = [] for layer_output in\nall_layer_outputs: final_output = reshape_from_matrix(layer_output,\ninput_shape) final_outputs.append(final_output) return final_outputs\nelse: final_output = reshape_from_matrix(prev_output, input_shape)\nreturn final_output</p>\n<p>def get_shape_list(tensor, expected_rank=None, name=None): \"\"\"Returns\na list of the shape of tensor, preferring static dimensions.</p>\n<p>Args: tensor: A tf.Tensor object to find the shape of. expected_rank:\n(optional) int. The expected rank of <code>tensor</code>. If this is\nspecified and the <code>tensor</code> has a different rank, and\nexception will be thrown. name: Optional name of the tensor for the\nerror message.</p>\n<p>Returns: A list of dimensions of the shape of tensor. All static\ndimensions will be returned as python integers, and dynamic dimensions\nwill be returned as tf.Tensor scalars. \"\"\" if name is None: name =\ntensor.name</p>\n<p>if expected_rank is not None: assert_rank(tensor, expected_rank,\nname)</p>\n<p>shape = tensor.shape.as_list()</p>\n<p>non_static_indexes = [] for (index, dim) in enumerate(shape): if dim\nis None: non_static_indexes.append(index)</p>\n<p>if not non_static_indexes: return shape</p>\n<p>dyn_shape = tf.shape(tensor) for index in non_static_indexes:\nshape[index] = dyn_shape[index] return shape</p>\n<p>def reshape_to_matrix(input_tensor): \"\"\"Reshapes a &gt;= rank 2\ntensor to a rank 2 tensor (i.e., a matrix).\"\"\" ndims =\ninput_tensor.shape.ndims if ndims &lt; 2: raise ValueError(\"Input tensor\nmust have at least rank 2. Shape = %s\" % (input_tensor.shape)) if ndims\n== 2: return input_tensor</p>\n<p>width = input_tensor.shape[-1] output_tensor =\ntf.reshape(input_tensor, [-1, width]) return output_tensor</p>\n<p>def reshape_from_matrix(output_tensor, orig_shape_list): \"\"\"Reshapes\na rank 2 tensor back to its original rank &gt;= 2 tensor.\"\"\" if\nlen(orig_shape_list) == 2: return output_tensor</p>\n<p>output_shape = get_shape_list(output_tensor)</p>\n<p>orig_dims = orig_shape_list[0:-1] width = output_shape[-1]</p>\n<p>return tf.reshape(output_tensor, orig_dims + [width])</p>\n<p>def assert_rank(tensor, expected_rank, name=None): \"\"\"Raises an\nexception if the tensor rank is not of the expected rank.</p>\n<p>Args: tensor: A tf.Tensor to check the rank of. expected_rank: Python\ninteger or list of integers, expected rank. name: Optional name of the\ntensor for the error message.</p>\n<p>Raises: ValueError: If the expected shape doesn't match the actual\nshape. \"\"\" if name is None: name = tensor.name</p>\n<p>expected_rank_dict = {} if isinstance(expected_rank,\nsix.integer_types): expected_rank_dict[expected_rank] = True else: for x\nin expected_rank: expected_rank_dict[x] = True</p>\n<p>actual_rank = tensor.shape.ndims if actual_rank not in\nexpected_rank_dict: scope_name = tf.compat.v1.get_variable_scope().name\nraise ValueError( \"For the tensor <code>%s</code> in scope\n<code>%s</code>, the actual rank \" \"<code>%d</code> (shape = %s) is not\nequal to the expected rank <code>%s</code>\" % (name, scope_name,\nactual_rank, str(tensor.shape), str(expected_rank)))</p>\n<p>```</p>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","more":"<h1 id=\"bert-tf-2.6修改-自调适配版\">Bert-TF-2.6修改-自调适配版</h1>\n<h4 id=\"背景\">背景</h4>\n<p>bert开源版适配的为tf1版本，当机器的tf环境为2以上版本时，会出现各种异常。</p>\n<p>因此我根据tf的函数库，进行了bert适配tf2.6的修改适配。</p>\n<h4 id=\"代码\">代码</h4>\n<h5 id=\"run_classifier.py\">run_classifier.py</h5>\n<pre class=\"python3\"><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport os\nimport modeling\nimport optimization\nimport tokenization\nimport tensorflow as tf\nfrom absl import flags\nfrom absl import app\nimport pickle\n\n# flags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    &quot;data_dir&quot;, None,\n    &quot;The input data dir. Should contain the .tsv files (or other data files) &quot;\n    &quot;for the task.&quot;)\n\nflags.DEFINE_string(\n    &quot;bert_config_file&quot;, None,\n    &quot;The config json file corresponding to the pre-trained BERT model. &quot;\n    &quot;This specifies the model architecture.&quot;)\n\nflags.DEFINE_string(&quot;task_name&quot;, None, &quot;The name of the task to train.&quot;)\n\nflags.DEFINE_string(&quot;vocab_file&quot;, None,\n                    &quot;The vocabulary file that the BERT model was trained on.&quot;)\n\nflags.DEFINE_string(\n    &quot;output_dir&quot;, None,\n    &quot;The output directory where the model checkpoints will be written.&quot;)\n\nflags.DEFINE_string(\n    &quot;trans_model_dir&quot;, None,\n    &quot;The trans_model_dir directory where the model will be written.&quot;)\n\n\n## Other parameters\n\nflags.DEFINE_string(\n    &quot;init_checkpoint&quot;, None,\n    &quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;)\n\nflags.DEFINE_bool(\n    &quot;do_lower_case&quot;, True,\n    &quot;Whether to lower case the input text. Should be True for uncased &quot;\n    &quot;models and False for cased models.&quot;)\n\nflags.DEFINE_integer(\n    &quot;max_seq_length&quot;, 128,\n    &quot;The maximum total input sequence length after WordPiece tokenization. &quot;\n    &quot;Sequences longer than this will be truncated, and sequences shorter &quot;\n    &quot;than this will be padded.&quot;)\n\nflags.DEFINE_bool(&quot;do_train&quot;, False, &quot;Whether to run training.&quot;)\n\nflags.DEFINE_bool(&quot;do_eval&quot;, False, &quot;Whether to run eval on the dev set.&quot;)\n\nflags.DEFINE_bool(\n    &quot;do_predict&quot;, False,\n    &quot;Whether to run the model in inference mode on the test set.&quot;)\n\nflags.DEFINE_integer(&quot;train_batch_size&quot;, 32, &quot;Total batch size for training.&quot;)\n\nflags.DEFINE_integer(&quot;eval_batch_size&quot;, 8, &quot;Total batch size for eval.&quot;)\n\nflags.DEFINE_integer(&quot;predict_batch_size&quot;, 8, &quot;Total batch size for predict.&quot;)\n\nflags.DEFINE_float(&quot;learning_rate&quot;, 5e-5, &quot;The initial learning rate for Adam.&quot;)\n\nflags.DEFINE_float(&quot;num_train_epochs&quot;, 3.0,\n                   &quot;Total number of training epochs to perform.&quot;)\n\nflags.DEFINE_float(\n    &quot;warmup_proportion&quot;, 0.1,\n    &quot;Proportion of training to perform linear learning rate warmup for. &quot;\n    &quot;E.g., 0.1 = 10% of training.&quot;)\n\nflags.DEFINE_integer(&quot;save_checkpoints_steps&quot;, 1000,\n                     &quot;How often to save the model checkpoint.&quot;)\n\nflags.DEFINE_integer(&quot;iterations_per_loop&quot;, 1000,\n                     &quot;How many steps to make in each estimator call.&quot;)\n\nflags.DEFINE_bool(&quot;use_tpu&quot;, False, &quot;Whether to use TPU or GPU/CPU.&quot;)\n\nflags.DEFINE_string(\n    &quot;tpu_name&quot;, None,\n    &quot;The Cloud TPU to use for training. This should be either the name &quot;\n    &quot;used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 &quot;\n    &quot;url.&quot;)\n\nflags.DEFINE_string(\n    &quot;tpu_zone&quot;, None,\n    &quot;[Optional] GCE zone where the Cloud TPU is located in. If not &quot;\n    &quot;specified, we will attempt to automatically detect the GCE project from &quot;\n    &quot;metadata.&quot;)\n\nflags.DEFINE_string(\n    &quot;gcp_project&quot;, None,\n    &quot;[Optional] Project name for the Cloud TPU-enabled project. If not &quot;\n    &quot;specified, we will attempt to automatically detect the GCE project from &quot;\n    &quot;metadata.&quot;)\n\nflags.DEFINE_string(&quot;master&quot;, None, &quot;[Optional] TensorFlow master URL.&quot;)\n\nflags.DEFINE_integer(\n    &quot;num_tpu_cores&quot;, 8,\n    &quot;Only used if `use_tpu` is True. Total number of TPU cores to use.&quot;)\n\n\nclass InputExample(object):\n  &quot;&quot;&quot;A single training/test example for simple sequence classification.&quot;&quot;&quot;\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    &quot;&quot;&quot;Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    &quot;&quot;&quot;\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass PaddingInputExample(object):\n  &quot;&quot;&quot;Fake example so the num input examples is a multiple of the batch size.\n\n  When running eval/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won&#39;t be generated.\n\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  &quot;&quot;&quot;\n\n\nclass InputFeatures(object):\n  &quot;&quot;&quot;A single set of features of data.&quot;&quot;&quot;\n\n  def __init__(self,\n               input_ids,\n               input_mask,\n               segment_ids,\n               label_id,\n               is_real_example=True):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.segment_ids = segment_ids\n    self.label_id = label_id\n    self.is_real_example = is_real_example\n\n\nclass DataProcessor(object):\n  &quot;&quot;&quot;Base class for data converters for sequence classification data sets.&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for the train set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for the dev set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;Gets a collection of `InputExample`s for prediction.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  def get_labels(self):\n    &quot;&quot;&quot;Gets the list of labels for this data set.&quot;&quot;&quot;\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    &quot;&quot;&quot;Reads a tab separated value file.&quot;&quot;&quot;\n    with tf.io.gfile.GFile(input_file, &quot;r&quot;) as f:\n      reader = csv.reader(f, delimiter=&quot;\\t&quot;, quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\nclass XnliProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the XNLI data set.&quot;&quot;&quot;\n\n  def __init__(self):\n    self.language = &quot;zh&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    lines = self._read_tsv(\n        os.path.join(data_dir, &quot;multinli&quot;,\n                     &quot;multinli.train.%s.tsv&quot; % self.language))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;train-%d&quot; % (i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[2])\n      if label == tokenization.convert_to_unicode(&quot;contradictory&quot;):\n        label = tokenization.convert_to_unicode(&quot;contradiction&quot;)\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    lines = self._read_tsv(os.path.join(data_dir, &quot;xnli.dev.tsv&quot;))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;dev-%d&quot; % (i)\n      language = tokenization.convert_to_unicode(line[0])\n      if language != tokenization.convert_to_unicode(self.language):\n        continue\n      text_a = tokenization.convert_to_unicode(line[6])\n      text_b = tokenization.convert_to_unicode(line[7])\n      label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;contradiction&quot;, &quot;entailment&quot;, &quot;neutral&quot;]\n\n\nclass MnliProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the MultiNLI data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev_matched.tsv&quot;)),\n        &quot;dev_matched&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test_matched.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;contradiction&quot;, &quot;entailment&quot;, &quot;neutral&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, tokenization.convert_to_unicode(line[0]))\n      text_a = tokenization.convert_to_unicode(line[8])\n      text_b = tokenization.convert_to_unicode(line[9])\n      if set_type == &quot;test&quot;:\n        label = &quot;contradiction&quot;\n      else:\n        label = tokenization.convert_to_unicode(line[-1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass MrpcProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the MRPC data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;0&quot;, &quot;1&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[3])\n      text_b = tokenization.convert_to_unicode(line[4])\n      if set_type == &quot;test&quot;:\n        label = &quot;0&quot;\n      else:\n        label = tokenization.convert_to_unicode(line[0])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass ColaProcessor(DataProcessor):\n  &quot;&quot;&quot;Processor for the CoLA data set (GLUE version).&quot;&quot;&quot;\n\n  def get_train_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)\n\n  def get_dev_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)\n\n  def get_test_examples(self, data_dir):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;)\n\n  def get_labels(self):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    return [&quot;0&quot;, &quot;1&quot;]\n\n  def _create_examples(self, lines, set_type):\n    &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;\n    examples = []\n    for (i, line) in enumerate(lines):\n      # Only the test set has a header\n      if set_type == &quot;test&quot; and i == 0:\n        continue\n      guid = &quot;%s-%s&quot; % (set_type, i)\n      if set_type == &quot;test&quot;:\n        text_a = tokenization.convert_to_unicode(line[1])\n        label = &quot;0&quot;\n      else:\n        text_a = tokenization.convert_to_unicode(line[3])\n        label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  &quot;&quot;&quot;Converts a single `InputExample` into a single `InputFeatures`.&quot;&quot;&quot;\n\n  if isinstance(example, PaddingInputExample):\n    return InputFeatures(\n        input_ids=[0] * max_seq_length,\n        input_mask=[0] * max_seq_length,\n        segment_ids=[0] * max_seq_length,\n        label_id=0,\n        is_real_example=False)\n\n  label_map = &#123;&#125;\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  output_label2id_file = os.path.join(FLAGS.trans_model_dir, &quot;label2id.pkl&quot;)\n  if not os.path.exists(output_label2id_file):\n    with open(output_label2id_file, &#39;wb&#39;) as w:\n        pickle.dump(label_map, w)\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with &quot;- 3&quot;\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with &quot;- 2&quot;\n    if len(tokens_a) &gt; max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where &quot;type_ids&quot; are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as the &quot;sentence vector&quot;. Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(&quot;[CLS]&quot;)\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(&quot;[SEP]&quot;)\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(&quot;[SEP]&quot;)\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) &lt; max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index &lt; 5:\n    tf.compat.v1.logging.info(&quot;*** Example ***&quot;)\n    tf.compat.v1.logging.info(&quot;guid: %s&quot; % (example.guid))\n    tf.compat.v1.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.compat.v1.logging.info(&quot;input_ids: %s&quot; % &quot; &quot;.join([str(x) for x in input_ids]))\n    tf.compat.v1.logging.info(&quot;input_mask: %s&quot; % &quot; &quot;.join([str(x) for x in input_mask]))\n    tf.compat.v1.logging.info(&quot;segment_ids: %s&quot; % &quot; &quot;.join([str(x) for x in segment_ids]))\n    tf.compat.v1.logging.info(&quot;label: %s (id = %d)&quot; % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id,\n      is_real_example=True)\n  return feature\n\n\ndef file_based_convert_examples_to_features(\n    examples, label_list, max_seq_length, tokenizer, output_file):\n  &quot;&quot;&quot;Convert a set of `InputExample`s to a TFRecord file.&quot;&quot;&quot;\n\n  writer = tf.io.TFRecordWriter(output_file)\n\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.compat.v1.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    def create_int_feature(values):\n      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n      return f\n\n    features = collections.OrderedDict()\n    features[&quot;input_ids&quot;] = create_int_feature(feature.input_ids)\n    features[&quot;input_mask&quot;] = create_int_feature(feature.input_mask)\n    features[&quot;segment_ids&quot;] = create_int_feature(feature.segment_ids)\n    features[&quot;label_ids&quot;] = create_int_feature([feature.label_id])\n    features[&quot;is_real_example&quot;] = create_int_feature(\n        [int(feature.is_real_example)])\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    writer.write(tf_example.SerializeToString())\n  writer.close()\n\n\ndef file_based_input_fn_builder(input_file, seq_length, is_training,\n                                drop_remainder):\n  &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot;\n\n  name_to_features = &#123;\n      &quot;input_ids&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;input_mask&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;segment_ids&quot;: tf.io.FixedLenFeature([seq_length], tf.int64),\n      &quot;label_ids&quot;: tf.io.FixedLenFeature([], tf.int64),\n      &quot;is_real_example&quot;: tf.io.FixedLenFeature([], tf.int64),\n  &#125;\n\n  def _decode_record(record, name_to_features):\n    &quot;&quot;&quot;Decodes a record to a TensorFlow example.&quot;&quot;&quot;\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.compat.v1.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    &quot;&quot;&quot;The actual input function.&quot;&quot;&quot;\n    batch_size = params[&quot;batch_size&quot;]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn&#39;t matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.data.experimental.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n\n    return d\n\n  return input_fn\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  &quot;&quot;&quot;Truncates a sequence pair in place to the maximum length.&quot;&quot;&quot;\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that&#39;s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length &lt;= max_length:\n      break\n    if len(tokens_a) &gt; len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  &quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot;\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output()\n\n  hidden_size = output_layer.shape[-1]\n\n  output_weights = tf.compat.v1.get_variable(\n      &quot;output_weights&quot;, [num_labels, hidden_size],\n      initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.compat.v1.get_variable(\n      &quot;output_bias&quot;, [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.compat.v1.variable_scope(&quot;loss&quot;):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.compat.v1.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  &quot;&quot;&quot;Returns `model_fn` closure for TPUEstimator.&quot;&quot;&quot;\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    &quot;&quot;&quot;The `model_fn` for TPUEstimator.&quot;&quot;&quot;\n\n    tf.compat.v1.logging.info(&quot;*** Features ***&quot;)\n    for name in sorted(features.keys()):\n      tf.compat.v1.logging.info(&quot;  name = %s, shape = %s&quot; % (name, features[name].shape))\n\n    input_ids = features[&quot;input_ids&quot;]\n    input_mask = features[&quot;input_mask&quot;]\n    segment_ids = features[&quot;segment_ids&quot;]\n    label_ids = features[&quot;label_ids&quot;]\n    is_real_example = None\n    if &quot;is_real_example&quot; in features:\n      is_real_example = tf.cast(features[&quot;is_real_example&quot;], dtype=tf.float32)\n    else:\n      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    (total_loss, per_example_loss, logits, probabilities) = create_model(\n        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n        num_labels, use_one_hot_embeddings)\n\n    tvars = tf.compat.v1.trainable_variables()\n    initialized_variable_names = &#123;&#125;\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.compat.v1.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.compat.v1.logging.info(&quot;**** Trainable Variables ****&quot;)\n    for var in tvars:\n      init_string = &quot;&quot;\n      if var.name in initialized_variable_names:\n        init_string = &quot;, *INIT_FROM_CKPT*&quot;\n      tf.compat.v1.logging.info(&quot;  name = %s, shape = %s%s&quot;, var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        accuracy = tf.compat.v1.metrics.accuracy(\n            labels=label_ids, predictions=predictions, weights=is_real_example)\n        loss = tf.compat.v1.metrics.mean(values=per_example_loss, weights=is_real_example)\n        return &#123;\n            &quot;eval_accuracy&quot;: accuracy,\n            &quot;eval_loss&quot;: loss,\n        &#125;\n\n      eval_metrics = (metric_fn,\n                      [per_example_loss, label_ids, logits, is_real_example])\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions=&#123;&quot;probabilities&quot;: probabilities&#125;,\n          scaffold_fn=scaffold_fn)\n    return output_spec\n\n  return model_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot;\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    &quot;&quot;&quot;The actual input function.&quot;&quot;&quot;\n    batch_size = params[&quot;batch_size&quot;]\n\n    num_examples = len(features)\n\n    # This is for demo purposes and does NOT scale to large data sets. We do\n    # not use Dataset.from_generator() because that uses tf.py_func which is\n    # not TPU compatible. The right way to load data is with TFRecordReader.\n    d = tf.data.Dataset.from_tensor_slices(&#123;\n        &quot;input_ids&quot;:\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;input_mask&quot;:\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;segment_ids&quot;:\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        &quot;label_ids&quot;:\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    &#125;)\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer):\n  &quot;&quot;&quot;Convert a set of `InputExample`s to a list of `InputFeatures`.&quot;&quot;&quot;\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.compat.v1.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n\ndef serving_input_fn():\n    # 保存模型为SaveModel格式\n    # 采用最原始的feature方式，输入是feature Tensors。\n    # 如果采用build_parsing_serving_input_receiver_fn，则输入是tf.Examples\n\n    label_ids = tf.compat.v1.placeholder(tf.int32, [None, 3], name=&#39;label_ids&#39;)\n    input_ids = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;input_ids&#39;)\n    input_mask = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;input_mask&#39;)\n    segment_ids = tf.compat.v1.placeholder(tf.int32, [None, 200], name=&#39;segment_ids&#39;)\n    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(&#123;\n        &#39;label_ids&#39;: label_ids,\n        &#39;input_ids&#39;: input_ids,\n        &#39;input_mask&#39;: input_mask,\n        &#39;segment_ids&#39;: segment_ids,\n    &#125;)()\n    return input_fn\n\ndef main(_):\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n\n  processors = &#123;\n      &quot;cola&quot;: ColaProcessor,\n      &quot;mnli&quot;: MnliProcessor,\n      &quot;mrpc&quot;: MrpcProcessor,\n      &quot;xnli&quot;: XnliProcessor,\n  &#125;\n\n  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n                                                FLAGS.init_checkpoint)\n\n  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n    raise ValueError(\n        &quot;At least one of `do_train`, `do_eval` or `do_predict&#39; must be True.&quot;)\n\n  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n  if FLAGS.max_seq_length &gt; bert_config.max_position_embeddings:\n    raise ValueError(\n        &quot;Cannot use sequence length %d because the BERT model &quot;\n        &quot;was only trained up to sequence length %d&quot; %\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf.io.gfile.makedirs(FLAGS.trans_model_dir)\n\n  task_name = FLAGS.task_name.lower()\n\n  if task_name not in processors:\n    raise ValueError(&quot;Task not found: %s&quot; % (task_name))\n\n  processor = processors[task_name]()\n\n  label_list = processor.get_labels()\n\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:\n    train_examples = processor.get_train_examples(FLAGS.data_dir)\n    num_train_steps = int(\n        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n  model_fn = model_fn_builder(\n      bert_config=bert_config,\n      num_labels=len(label_list),\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size)\n\n  if FLAGS.do_train:\n    train_file = os.path.join(FLAGS.output_dir, &quot;train.tf_record&quot;)\n    file_based_convert_examples_to_features(\n        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n    tf.compat.v1.logging.info(&quot;***** Running training *****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d&quot;, len(train_examples))\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.train_batch_size)\n    tf.compat.v1.logging.info(&quot;  Num steps = %d&quot;, num_train_steps)\n    train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\n  if FLAGS.do_eval:\n    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n    num_actual_eval_examples = len(eval_examples)\n    if FLAGS.use_tpu:\n      # TPU requires a fixed batch size for all batches, therefore the number\n      # of examples must be a multiple of the batch size, or else examples\n      # will get dropped. So we pad with fake examples which are ignored\n      # later on. These do NOT count towards the metric (all tf.metrics\n      # support a per-instance weight, and these get a weight of 0.0).\n      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n        eval_examples.append(PaddingInputExample())\n\n    eval_file = os.path.join(FLAGS.output_dir, &quot;eval.tf_record&quot;)\n    file_based_convert_examples_to_features(\n        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n\n    tf.compat.v1.logging.info(&quot;***** Running evaluation *****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d (%d actual, %d padding)&quot;,\n                    len(eval_examples), num_actual_eval_examples,\n                    len(eval_examples) - num_actual_eval_examples)\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.eval_batch_size)\n\n    # This tells the estimator to run through the entire set.\n    eval_steps = None\n    # However, if running eval on the TPU, you will need to specify the\n    # number of steps.\n    if FLAGS.use_tpu:\n      assert len(eval_examples) % FLAGS.eval_batch_size == 0\n      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n\n    eval_drop_remainder = True if FLAGS.use_tpu else False\n    eval_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=eval_drop_remainder)\n\n    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n\n    # trans_model_dir模型转换后输出目录，将模型转换为saved model\n    estimator._export_to_tpu = False\n    estimator.export_savedmodel(FLAGS.trans_model_dir, serving_input_fn)\n\n    output_eval_file = os.path.join(FLAGS.output_dir, &quot;eval_results.txt&quot;)\n    with tf.io.gfile.GFile(output_eval_file, &quot;w&quot;) as writer:\n      tf.compat.v1.logging.info(&quot;***** Eval results *****&quot;)\n      for key in sorted(result.keys()):\n        tf.compat.v1.logging.info(&quot;  %s = %s&quot;, key, str(result[key]))\n        writer.write(&quot;%s = %s\\n&quot; % (key, str(result[key])))\n\n  if FLAGS.do_predict:\n    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n    num_actual_predict_examples = len(predict_examples)\n    if FLAGS.use_tpu:\n      # TPU requires a fixed batch size for all batches, therefore the number\n      # of examples must be a multiple of the batch size, or else examples\n      # will get dropped. So we pad with fake examples which are ignored\n      # later on.\n      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n        predict_examples.append(PaddingInputExample())\n\n    predict_file = os.path.join(FLAGS.output_dir, &quot;predict.tf_record&quot;)\n    file_based_convert_examples_to_features(predict_examples, label_list,\n                                            FLAGS.max_seq_length, tokenizer,\n                                            predict_file)\n\n    tf.compat.v1.logging.info(&quot;***** Running prediction*****&quot;)\n    tf.compat.v1.logging.info(&quot;  Num examples = %d (%d actual, %d padding)&quot;,\n                    len(predict_examples), num_actual_predict_examples,\n                    len(predict_examples) - num_actual_predict_examples)\n    tf.compat.v1.logging.info(&quot;  Batch size = %d&quot;, FLAGS.predict_batch_size)\n\n    predict_drop_remainder = True if FLAGS.use_tpu else False\n    predict_input_fn = file_based_input_fn_builder(\n        input_file=predict_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=predict_drop_remainder)\n\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    output_predict_file = os.path.join(FLAGS.output_dir, &quot;test_results.tsv&quot;)\n    with tf.io.gfile.GFile(output_predict_file, &quot;w&quot;) as writer:\n      num_written_lines = 0\n      tf.compat.v1.logging.info(&quot;***** Predict results *****&quot;)\n      for (i, prediction) in enumerate(result):\n        probabilities = prediction[&quot;probabilities&quot;]\n        if i &gt;= num_actual_predict_examples:\n          break\n        output_line = &quot;\\t&quot;.join(\n            str(class_probability)\n            for class_probability in probabilities) + &quot;\\n&quot;\n        writer.write(output_line)\n        num_written_lines += 1\n    assert num_written_lines == num_actual_predict_examples\n\n\nif __name__ == &quot;__main__&quot;:\n  flags.mark_flag_as_required(&quot;data_dir&quot;)\n  flags.mark_flag_as_required(&quot;task_name&quot;)\n  flags.mark_flag_as_required(&quot;vocab_file&quot;)\n  flags.mark_flag_as_required(&quot;bert_config_file&quot;)\n  flags.mark_flag_as_required(&quot;output_dir&quot;)\n  flags.mark_flag_as_required(&quot;trans_model_dir&quot;)\n  app.run(main)</code></pre>\n<h5 id=\"optimization.py\">optimization.py</h5>\n<pre class=\"python3\"><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n  &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot;\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n\n  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n  # Implements linear decay of the learning rate.\n  learning_rate = tf.compat.v1.train.polynomial_decay(\n      learning_rate,\n      global_step,\n      num_train_steps,\n      end_learning_rate=0.0,\n      power=1.0,\n      cycle=False)\n\n  # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the\n  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n  if num_warmup_steps:\n    global_steps_int = tf.cast(global_step, tf.int32)\n    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n    global_steps_float = tf.cast(global_steps_int, tf.float32)\n    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n    warmup_percent_done = global_steps_float / warmup_steps_float\n    warmup_learning_rate = init_lr * warmup_percent_done\n\n    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)\n    learning_rate = (\n        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n  # It is recommended that you use this optimizer for fine tuning, since this\n  # is how the model was trained (note that the Adam m/v variables are NOT\n  # loaded from init_checkpoint.)\n  optimizer = AdamWeightDecayOptimizer(\n      learning_rate=learning_rate,\n      weight_decay_rate=0.01,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])\n\n  if use_tpu:\n    optimizer = tf.compat.v1.estimator.tpu.CrossShardOptimizer(optimizer)\n\n  tvars = tf.compat.v1.trainable_variables()\n  grads = tf.gradients(loss, tvars)\n\n  # This is how the model was pre-trained.\n  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n\n  train_op = optimizer.apply_gradients(\n      zip(grads, tvars), global_step=global_step)\n\n  # Normally the global step update is done inside of `apply_gradients`.\n  # However, `AdamWeightDecayOptimizer` doesn&#39;t do this. But if you use\n  # a different optimizer, you should probably take this line out.\n  new_global_step = global_step + 1\n  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n  return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.keras.optimizers.Optimizer):\n  &quot;&quot;&quot;A basic Adam optimizer that includes &quot;correct&quot; L2 weight decay.&quot;&quot;&quot;\n\n  def __init__(self,\n               learning_rate,\n               weight_decay_rate=0.0,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-6,\n               exclude_from_weight_decay=None,\n               name=&quot;AdamWeightDecayOptimizer&quot;):\n    &quot;&quot;&quot;Constructs a AdamWeightDecayOptimizer.&quot;&quot;&quot;\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    &quot;&quot;&quot;See base class.&quot;&quot;&quot;\n    assignments = []\n    for (grad, param) in grads_and_vars:\n      if grad is None or param is None:\n        continue\n\n      param_name = self._get_variable_name(param.name)\n\n      m = tf.compat.v1.get_variable(\n          name=param_name + &quot;/adam_m&quot;,\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n      v = tf.compat.v1.get_variable(\n          name=param_name + &quot;/adam_v&quot;,\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n\n      # Standard Adam update.\n      next_m = (\n          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n      next_v = (\n          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                    tf.square(grad)))\n\n      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n      # Just adding the square of the weights to the loss function is *not*\n      # the correct way of using L2 regularization/weight decay with Adam,\n      # since that will interact with the m and v parameters in strange ways.\n      #\n      # Instead we want ot decay the weights in a manner that doesn&#39;t interact\n      # with the m/v parameters. This is equivalent to adding the square\n      # of the weights to the loss with plain (non-momentum) SGD.\n      if self._do_use_weight_decay(param_name):\n        update += self.weight_decay_rate * param\n\n      update_with_lr = self.learning_rate * update\n\n      next_param = param - update_with_lr\n\n      assignments.extend(\n          [param.assign(next_param),\n           m.assign(next_m),\n           v.assign(next_v)])\n    return tf.group(*assignments, name=name)\n\n  def _do_use_weight_decay(self, param_name):\n    &quot;&quot;&quot;Whether to use L2 weight decay for `param_name`.&quot;&quot;&quot;\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n\n  def _get_variable_name(self, param_name):\n    &quot;&quot;&quot;Get the variable name from the tensor name.&quot;&quot;&quot;\n    m = re.match(&quot;^(.*):\\\\d+$&quot;, param_name)\n    if m is not None:\n      param_name = m.group(1)\n    return param_name\n</code></pre>\n<h5 id=\"tokenization.py\">tokenization.py</h5>\n<pre class=\"python3\"><code>\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n  &quot;&quot;&quot;Checks whether the casing config is consistent with the checkpoint name.&quot;&quot;&quot;\n\n  # The casing has to be passed in by the user and there is no explicit check\n  # as to whether it matches the checkpoint. The casing information probably\n  # should have been stored in the bert_config.json file, but it&#39;s not, so\n  # we have to heuristically detect it to validate.\n\n  if not init_checkpoint:\n    return\n\n  m = re.match(&quot;^.*?([A-Za-z0-9_-]+)/bert_model.ckpt&quot;, init_checkpoint)\n  if m is None:\n    return\n\n  model_name = m.group(1)\n\n  lower_models = [\n      &quot;uncased_L-24_H-1024_A-16&quot;, &quot;uncased_L-12_H-768_A-12&quot;,\n      &quot;multilingual_L-12_H-768_A-12&quot;, &quot;chinese_L-12_H-768_A-12&quot;\n  ]\n\n  cased_models = [\n      &quot;cased_L-12_H-768_A-12&quot;, &quot;cased_L-24_H-1024_A-16&quot;,\n      &quot;multi_cased_L-12_H-768_A-12&quot;\n  ]\n\n  is_bad_config = False\n  if model_name in lower_models and not do_lower_case:\n    is_bad_config = True\n    actual_flag = &quot;False&quot;\n    case_name = &quot;lowercased&quot;\n    opposite_flag = &quot;True&quot;\n\n  if model_name in cased_models and do_lower_case:\n    is_bad_config = True\n    actual_flag = &quot;True&quot;\n    case_name = &quot;cased&quot;\n    opposite_flag = &quot;False&quot;\n\n  if is_bad_config:\n    raise ValueError(\n        &quot;You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. &quot;\n        &quot;However, `%s` seems to be a %s model, so you &quot;\n        &quot;should pass in `--do_lower_case=%s` so that the fine-tuning matches &quot;\n        &quot;how the model was pre-training. If this error is wrong, please &quot;\n        &quot;just comment out this check.&quot; % (actual_flag, init_checkpoint,\n                                          model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n  &quot;&quot;&quot;Converts `text` to Unicode (if it&#39;s not already), assuming utf-8 input.&quot;&quot;&quot;\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  else:\n    raise ValueError(&quot;Not running on Python2 or Python 3?&quot;)\n\n\ndef printable_text(text):\n  &quot;&quot;&quot;Returns text encoded in a way suitable for print or `tf.logging`.&quot;&quot;&quot;\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it&#39;s a Unicode string and in the other it&#39;s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(&quot;utf-8&quot;, &quot;ignore&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(&quot;utf-8&quot;)\n    else:\n      raise ValueError(&quot;Unsupported string type: %s&quot; % (type(text)))\n  else:\n    raise ValueError(&quot;Not running on Python2 or Python 3?&quot;)\n\n\ndef load_vocab(vocab_file):\n  &quot;&quot;&quot;Loads a vocabulary file into a dictionary.&quot;&quot;&quot;\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.io.gfile.GFile(vocab_file, &quot;r&quot;) as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  &quot;&quot;&quot;Converts a sequence of [tokens|ids] using the vocab.&quot;&quot;&quot;\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  &quot;&quot;&quot;Runs basic whitespace cleaning and splitting on a piece of text.&quot;&quot;&quot;\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  &quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125;\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  &quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;\n\n  def __init__(self, do_lower_case=True):\n    &quot;&quot;&quot;Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    &quot;&quot;&quot;\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    &quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn&#39;t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don&#39;t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    &quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;\n    text = unicodedata.normalize(&quot;NFD&quot;, text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == &quot;Mn&quot;:\n        continue\n      output.append(char)\n    return &quot;&quot;.join(output)\n\n  def _run_split_on_punc(self, text):\n    &quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i &lt; len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return [&quot;&quot;.join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    &quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append(&quot; &quot;)\n        output.append(char)\n        output.append(&quot; &quot;)\n      else:\n        output.append(char)\n    return &quot;&quot;.join(output)\n\n  def _is_chinese_char(self, cp):\n    &quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;\n    # This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or  #\n        (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or  #\n        (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or  #\n        (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or  #\n        (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or  #\n        (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or\n        (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or  #\n        (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    &quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append(&quot; &quot;)\n      else:\n        output.append(char)\n    return &quot;&quot;.join(output)\n\n\nclass WordpieceTokenizer(object):\n  &quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;\n\n  def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    &quot;&quot;&quot;Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = &quot;unaffable&quot;\n      output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    &quot;&quot;&quot;\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) &gt; self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start &lt; len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start &lt; end:\n          substr = &quot;&quot;.join(chars[start:end])\n          if start &gt; 0:\n            substr = &quot;##&quot; + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  &quot;&quot;&quot;Checks whether `chars` is a whitespace character.&quot;&quot;&quot;\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == &quot; &quot; or char == &quot;\\t&quot; or char == &quot;\\n&quot; or char == &quot;\\r&quot;:\n    return True\n  cat = unicodedata.category(char)\n  if cat == &quot;Zs&quot;:\n    return True\n  return False\n\n\ndef _is_control(char):\n  &quot;&quot;&quot;Checks whether `chars` is a control character.&quot;&quot;&quot;\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == &quot;\\t&quot; or char == &quot;\\n&quot; or char == &quot;\\r&quot;:\n    return False\n  cat = unicodedata.category(char)\n  if cat in (&quot;Cc&quot;, &quot;Cf&quot;):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  &quot;&quot;&quot;Checks whether `chars` is a punctuation character.&quot;&quot;&quot;\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as &quot;^&quot;, &quot;$&quot;, and &quot;`&quot; are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp &gt;= 33 and cp &lt;= 47) or (cp &gt;= 58 and cp &lt;= 64) or\n      (cp &gt;= 91 and cp &lt;= 96) or (cp &gt;= 123 and cp &lt;= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(&quot;P&quot;):\n    return True\n  return False\n</code></pre>\n<h5 id=\"modeling.py\">modeling.py</h5>\n<pre class=\"python3\"><code>\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  &quot;&quot;&quot;Configuration for `BertModel`.&quot;&quot;&quot;\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=&quot;gelu&quot;,\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    &quot;&quot;&quot;Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the &quot;intermediate&quot; (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    &quot;&quot;&quot;\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    &quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot;\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    &quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot;\n    with tf.io.gfile.GFile(json_file, &quot;r&quot;) as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    &quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot;\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    &quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot;\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\\n&quot;\n\n\nclass BertModel(object):\n  &quot;&quot;&quot;BERT model (&quot;Bidirectional Encoder Representations from Transformers&quot;).\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.compact.v1.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...</code></pre>\n<p>\"\"\"</p>\n<p>def <strong>init</strong>(self, config, is_training, input_ids,\ninput_mask=None, token_type_ids=None, use_one_hot_embeddings=False,\nscope=None): \"\"\"Constructor for BertModel.</p>\n<pre><code>Args:\n  config: `BertConfig` instance.\n  is_training: bool. true for training model, false for eval model. Controls\n    whether dropout will be applied.\n  input_ids: int32 Tensor of shape [batch_size, seq_length].\n  input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n  token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n  use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n    embeddings or tf.embedding_lookup() for the word embeddings.\n  scope: (optional) variable scope. Defaults to &quot;bert&quot;.\n\nRaises:\n  ValueError: The config is invalid or one of the input tensor shapes\n    is invalid.\n&quot;&quot;&quot;\nconfig = copy.deepcopy(config)\nif not is_training:\n  config.hidden_dropout_prob = 0.0\n  config.attention_probs_dropout_prob = 0.0\n\ninput_shape = get_shape_list(input_ids, expected_rank=2)\nbatch_size = input_shape[0]\nseq_length = input_shape[1]\n\nif input_mask is None:\n  input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\nif token_type_ids is None:\n  token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\nwith tf.compat.v1.variable_scope(scope, default_name=&quot;bert&quot;):\n  with tf.compat.v1.variable_scope(&quot;embeddings&quot;):\n    # Perform embedding lookup on the word ids.\n    (self.embedding_output, self.embedding_table) = embedding_lookup(\n        input_ids=input_ids,\n        vocab_size=config.vocab_size,\n        embedding_size=config.hidden_size,\n        initializer_range=config.initializer_range,\n        word_embedding_name=&quot;word_embeddings&quot;,\n        use_one_hot_embeddings=use_one_hot_embeddings)\n\n    # Add positional embeddings and token type embeddings, then layer\n    # normalize and perform dropout.\n    self.embedding_output = embedding_postprocessor(\n        input_tensor=self.embedding_output,\n        use_token_type=True,\n        token_type_ids=token_type_ids,\n        token_type_vocab_size=config.type_vocab_size,\n        token_type_embedding_name=&quot;token_type_embeddings&quot;,\n        use_position_embeddings=True,\n        position_embedding_name=&quot;position_embeddings&quot;,\n        initializer_range=config.initializer_range,\n        max_position_embeddings=config.max_position_embeddings,\n        dropout_prob=config.hidden_dropout_prob)\n\n  with tf.compat.v1.variable_scope(&quot;encoder&quot;):\n    # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n    # mask of shape [batch_size, seq_length, seq_length] which is used\n    # for the attention scores.\n    attention_mask = create_attention_mask_from_input_mask(\n        input_ids, input_mask)\n\n    # Run the stacked transformer.\n    # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n    self.all_encoder_layers = transformer_model(\n        input_tensor=self.embedding_output,\n        attention_mask=attention_mask,\n        hidden_size=config.hidden_size,\n        num_hidden_layers=config.num_hidden_layers,\n        num_attention_heads=config.num_attention_heads,\n        intermediate_size=config.intermediate_size,\n        intermediate_act_fn=get_activation(config.hidden_act),\n        hidden_dropout_prob=config.hidden_dropout_prob,\n        attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n        initializer_range=config.initializer_range,\n        do_return_all_layers=True)\n\n  self.sequence_output = self.all_encoder_layers[-1]\n  # The &quot;pooler&quot; converts the encoded sequence tensor of shape\n  # [batch_size, seq_length, hidden_size] to a tensor of shape\n  # [batch_size, hidden_size]. This is necessary for segment-level\n  # (or segment-pair-level) classification tasks where we need a fixed\n  # dimensional representation of the segment.\n  with tf.compat.v1.variable_scope(&quot;pooler&quot;):\n    # We &quot;pool&quot; the model by simply taking the hidden state corresponding\n    # to the first token. We assume that this has been pre-trained\n    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n    self.pooled_output = tf.compat.v1.layers.dense(\n        first_token_tensor,\n        config.hidden_size,\n        activation=tf.tanh,\n        kernel_initializer=create_initializer(config.initializer_range))</code></pre>\n<p>def get_pooled_output(self): return self.pooled_output</p>\n<p>def get_sequence_output(self): \"\"\"Gets final hidden layer of\nencoder.</p>\n<pre><code>Returns:\n  float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n  to the final hidden of the transformer encoder.\n&quot;&quot;&quot;\nreturn self.sequence_output</code></pre>\n<p>def get_all_encoder_layers(self): return self.all_encoder_layers</p>\n<p>def get_embedding_output(self): \"\"\"Gets output of the embedding\nlookup (i.e., input to the transformer).</p>\n<pre><code>Returns:\n  float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n  to the output of the embedding layer, after summing the word\n  embeddings with the positional embeddings and the token type embeddings,\n  then performing layer normalization. This is the input to the transformer.\n&quot;&quot;&quot;\nreturn self.embedding_output</code></pre>\n<p>def get_embedding_table(self): return self.embedding_table</p>\n<p>def gelu(x): \"\"\"Gaussian Error Linear Unit.</p>\n<p>This is a smoother version of the RELU. Original paper:\nhttps://arxiv.org/abs/1606.08415 Args: x: float Tensor to perform\nactivation.</p>\n<p>Returns: <code>x</code> with the GELU activation applied. \"\"\" cdf =\n0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x,\n3))))) return x * cdf</p>\n<p>def get_activation(activation_string): \"\"\"Maps a string to a Python\nfunction, e.g., \"relu\" =&gt; <code>tf.nn.relu</code>.</p>\n<p>Args: activation_string: String name of the activation function.</p>\n<p>Returns: A Python function corresponding to the activation function.\nIf <code>activation_string</code> is None, empty, or \"linear\", this will\nreturn None. If <code>activation_string</code> is not a string, it will\nreturn <code>activation_string</code>.</p>\n<p>Raises: ValueError: The <code>activation_string</code> does not\ncorrespond to a known activation. \"\"\"</p>\n<p># We assume that anything that\"s not a string is already an\nactivation # function, so we just return it. if not\nisinstance(activation_string, six.string_types): return\nactivation_string</p>\n<p>if not activation_string: return None</p>\n<p>act = activation_string.lower() if act == \"linear\": return None elif\nact == \"relu\": return tf.nn.relu elif act == \"gelu\": return gelu elif\nact == \"tanh\": return tf.tanh else: raise ValueError(\"Unsupported\nactivation: %s\" % act)</p>\n<p>def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n\"\"\"Compute the union of the current variables and checkpoint\nvariables.\"\"\" assignment_map = {} initialized_variable_names = {}</p>\n<p>name_to_variable = collections.OrderedDict() for var in tvars: name =\nvar.name m = re.match(\"^(.*):\\d+$\", name) if m is not None: name =\nm.group(1) name_to_variable[name] = var</p>\n<p>init_vars = tf.compat.v1.train.list_variables(init_checkpoint)</p>\n<p>assignment_map = collections.OrderedDict() for x in init_vars: (name,\nvar) = (x[0], x[1]) if name not in name_to_variable: continue\nassignment_map[name] = name initialized_variable_names[name] = 1\ninitialized_variable_names[name + \":0\"] = 1</p>\n<p>return (assignment_map, initialized_variable_names)</p>\n<p>def dropout(input_tensor, dropout_prob): \"\"\"Perform dropout.</p>\n<p>Args: input_tensor: float Tensor. dropout_prob: Python float. The\nprobability of dropping out a value (NOT of <em>keeping</em> a dimension\nas in <code>tf.nn.dropout</code>).</p>\n<p>Returns: A version of <code>input_tensor</code> with dropout applied.\n\"\"\" if dropout_prob is None or dropout_prob == 0.0: return\ninput_tensor</p>\n<p>output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob) return\noutput</p>\n<p>def layer_norm(input_tensor, name=None): \"\"\"Run layer normalization\non the last dimension of the tensor.\"\"\" layernorm =\ntf.keras.layers.LayerNormalization(axis=-1) return\nlayernorm(input_tensor)</p>\n<p>def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n\"\"\"Runs layer normalization followed by dropout.\"\"\" output_tensor =\nlayer_norm(input_tensor, name) output_tensor = dropout(output_tensor,\ndropout_prob) return output_tensor</p>\n<p>def create_initializer(initializer_range=0.02): \"\"\"Creates a\n<code>truncated_normal_initializer</code> with the given range.\"\"\"\nreturn\ntf.compat.v1.truncated_normal_initializer(stddev=initializer_range)</p>\n<p>def embedding_lookup(input_ids, vocab_size, embedding_size=128,\ninitializer_range=0.02, word_embedding_name=\"word_embeddings\",\nuse_one_hot_embeddings=False): \"\"\"Looks up words embeddings for id\ntensor.</p>\n<p>Args: input_ids: int32 Tensor of shape [batch_size, seq_length]\ncontaining word ids. vocab_size: int. Size of the embedding vocabulary.\nembedding_size: int. Width of the word embeddings. initializer_range:\nfloat. Embedding initialization range. word_embedding_name: string. Name\nof the embedding table. use_one_hot_embeddings: bool. If True, use\none-hot method for word embeddings. If False, use\n<code>tf.gather()</code>.</p>\n<p>Returns: float Tensor of shape [batch_size, seq_length,\nembedding_size]. \"\"\" # This function assumes that the input is of shape\n[batch_size, seq_length, # num_inputs]. # # If the input is a 2D tensor\nof shape [batch_size, seq_length], we # reshape to [batch_size,\nseq_length, 1]. if input_ids.shape.ndims == 2: input_ids =\ntf.expand_dims(input_ids, axis=[-1])</p>\n<p>embedding_table = tf.compat.v1.get_variable(\nname=word_embedding_name, shape=[vocab_size, embedding_size],\ninitializer=create_initializer(initializer_range))</p>\n<p>flat_input_ids = tf.reshape(input_ids, [-1]) if\nuse_one_hot_embeddings: one_hot_input_ids = tf.one_hot(flat_input_ids,\ndepth=vocab_size) output = tf.matmul(one_hot_input_ids, embedding_table)\nelse: output = tf.gather(embedding_table, flat_input_ids)</p>\n<p>input_shape = get_shape_list(input_ids)</p>\n<p>output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] *\nembedding_size]) return (output, embedding_table)</p>\n<p>def embedding_postprocessor(input_tensor, use_token_type=False,\ntoken_type_ids=None, token_type_vocab_size=16,\ntoken_type_embedding_name=\"token_type_embeddings\",\nuse_position_embeddings=True,\nposition_embedding_name=\"position_embeddings\", initializer_range=0.02,\nmax_position_embeddings=512, dropout_prob=0.1): \"\"\"Performs various\npost-processing on a word embedding tensor.</p>\n<p>Args: input_tensor: float Tensor of shape [batch_size, seq_length,\nembedding_size]. use_token_type: bool. Whether to add embeddings for\n<code>token_type_ids</code>. token_type_ids: (optional) int32 Tensor of\nshape [batch_size, seq_length]. Must be specified if\n<code>use_token_type</code> is True. token_type_vocab_size: int. The\nvocabulary size of <code>token_type_ids</code>.\ntoken_type_embedding_name: string. The name of the embedding table\nvariable for token type ids. use_position_embeddings: bool. Whether to\nadd position embeddings for the position of each token in the sequence.\nposition_embedding_name: string. The name of the embedding table\nvariable for positional embeddings. initializer_range: float. Range of\nthe weight initialization. max_position_embeddings: int. Maximum\nsequence length that might ever be used with this model. This can be\nlonger than the sequence length of input_tensor, but cannot be shorter.\ndropout_prob: float. Dropout probability applied to the final output\ntensor.</p>\n<p>Returns: float tensor with same shape as\n<code>input_tensor</code>.</p>\n<p>Raises: ValueError: One of the tensor shapes or input values is\ninvalid. \"\"\" input_shape = get_shape_list(input_tensor, expected_rank=3)\nbatch_size = input_shape[0] seq_length = input_shape[1] width =\ninput_shape[2]</p>\n<p>output = input_tensor</p>\n<p>if use_token_type: if token_type_ids is None: raise\nValueError(\"<code>token_type_ids</code> must be specified if\"\n\"<code>use_token_type</code> is True.\") token_type_table =\ntf.compat.v1.get_variable( name=token_type_embedding_name,\nshape=[token_type_vocab_size, width],\ninitializer=create_initializer(initializer_range)) # This vocab will be\nsmall so we always do one-hot here, since it is always # faster for a\nsmall vocabulary. flat_token_type_ids = tf.reshape(token_type_ids, [-1])\none_hot_ids = tf.one_hot(flat_token_type_ids,\ndepth=token_type_vocab_size) token_type_embeddings =\ntf.matmul(one_hot_ids, token_type_table) token_type_embeddings =\ntf.reshape(token_type_embeddings, [batch_size, seq_length, width])\noutput += token_type_embeddings</p>\n<p>if use_position_embeddings: assert_op =\ntf.debugging.assert_less_equal(seq_length, max_position_embeddings) with\ntf.control_dependencies([assert_op]): full_position_embeddings =\ntf.compat.v1.get_variable( name=position_embedding_name,\nshape=[max_position_embeddings, width],\ninitializer=create_initializer(initializer_range)) # Since the position\nembedding table is a learned variable, we create it # using a (long)\nsequence length <code>max_position_embeddings</code>. The actual #\nsequence length might be shorter than this, for faster training of #\ntasks that do not have long sequences. # # So\n<code>full_position_embeddings</code> is effectively an embedding table\n# for position [0, 1, 2, ..., max_position_embeddings-1], and the\ncurrent # sequence has positions [0, 1, 2, ... seq_length-1], so we can\njust # perform a slice. position_embeddings =\ntf.slice(full_position_embeddings, [0, 0], [seq_length, -1]) num_dims =\nlen(output.shape.as_list())</p>\n<pre><code>  # Only the last two dimensions are relevant (`seq_length` and `width`), so\n  # we broadcast among the first dimensions, which is typically just\n  # the batch size.\n  position_broadcast_shape = []\n  for _ in range(num_dims - 2):\n    position_broadcast_shape.append(1)\n  position_broadcast_shape.extend([seq_length, width])\n  position_embeddings = tf.reshape(position_embeddings,\n                                   position_broadcast_shape)\n  output += position_embeddings</code></pre>\n<p>output = layer_norm_and_dropout(output, dropout_prob) return\noutput</p>\n<p>def create_attention_mask_from_input_mask(from_tensor, to_mask):\n\"\"\"Create 3D attention mask from a 2D tensor mask.</p>\n<p>Args: from_tensor: 2D or 3D Tensor of shape [batch_size,\nfrom_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size,\nto_seq_length].</p>\n<p>Returns: float Tensor of shape [batch_size, from_seq_length,\nto_seq_length]. \"\"\" from_shape = get_shape_list(from_tensor,\nexpected_rank=[2, 3]) batch_size = from_shape[0] from_seq_length =\nfrom_shape[1]</p>\n<p>to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length =\nto_shape[1]</p>\n<p>to_mask = tf.cast( tf.reshape(to_mask, [batch_size, 1,\nto_seq_length]), tf.float32)</p>\n<p># We don't assume that <code>from_tensor</code> is a mask (although\nit could be). We # don't actually care if we attend <em>from</em>\npadding tokens (only <em>to</em> padding) # tokens so we create a tensor\nof all ones. # # <code>broadcast_ones</code> = [batch_size,\nfrom_seq_length, 1] broadcast_ones = tf.ones( shape=[batch_size,\nfrom_seq_length, 1], dtype=tf.float32)</p>\n<p># Here we broadcast along two dimensions to create the mask. mask =\nbroadcast_ones * to_mask</p>\n<p>return mask</p>\n<p>def attention_layer(from_tensor, to_tensor, attention_mask=None,\nnum_attention_heads=1, size_per_head=512, query_act=None, key_act=None,\nvalue_act=None, attention_probs_dropout_prob=0.0,\ninitializer_range=0.02, do_return_2d_tensor=False, batch_size=None,\nfrom_seq_length=None, to_seq_length=None): \"\"\"Performs multi-headed\nattention from <code>from_tensor</code> to <code>to_tensor</code>.</p>\n<p>This is an implementation of multi-headed attention based on\n\"Attention is all you Need\". If <code>from_tensor</code> and\n<code>to_tensor</code> are the same, then this is self-attention. Each\ntimestep in <code>from_tensor</code> attends to the corresponding\nsequence in <code>to_tensor</code>, and returns a fixed-with vector.</p>\n<p>This function first projects <code>from_tensor</code> into a \"query\"\ntensor and <code>to_tensor</code> into \"key\" and \"value\" tensors. These\nare (effectively) a list of tensors of length\n<code>num_attention_heads</code>, where each tensor is of shape\n[batch_size, seq_length, size_per_head].</p>\n<p>Then, the query and key tensors are dot-producted and scaled. These\nare softmaxed to obtain attention probabilities. The value tensors are\nthen interpolated by these probabilities, then concatenated back to a\nsingle tensor and returned.</p>\n<p>In practice, the multi-headed attention are done with transposes and\nreshapes rather than actual separate tensors.</p>\n<p>Args: from_tensor: float Tensor of shape [batch_size,\nfrom_seq_length, from_width]. to_tensor: float Tensor of shape\n[batch_size, to_seq_length, to_width]. attention_mask: (optional) int32\nTensor of shape [batch_size, from_seq_length, to_seq_length]. The values\nshould be 1 or 0. The attention scores will effectively be set to\n-infinity for any positions in the mask that are 0, and will be\nunchanged for positions that are 1. num_attention_heads: int. Number of\nattention heads. size_per_head: int. Size of each attention head.\nquery_act: (optional) Activation function for the query transform.\nkey_act: (optional) Activation function for the key transform.\nvalue_act: (optional) Activation function for the value transform.\nattention_probs_dropout_prob: (optional) float. Dropout probability of\nthe attention probabilities. initializer_range: float. Range of the\nweight initializer. do_return_2d_tensor: bool. If True, the output will\nbe of shape [batch_size * from_seq_length, num_attention_heads *\nsize_per_head]. If False, the output will be of shape [batch_size,\nfrom_seq_length, num_attention_heads * size_per_head]. batch_size:\n(Optional) int. If the input is 2D, this might be the batch size of the\n3D version of the <code>from_tensor</code> and <code>to_tensor</code>.\nfrom_seq_length: (Optional) If the input is 2D, this might be the seq\nlength of the 3D version of the <code>from_tensor</code>. to_seq_length:\n(Optional) If the input is 2D, this might be the seq length of the 3D\nversion of the <code>to_tensor</code>.</p>\n<p>Returns: float Tensor of shape [batch_size, from_seq_length,\nnum_attention_heads * size_per_head]. (If\n<code>do_return_2d_tensor</code> is true, this will be of shape\n[batch_size * from_seq_length, num_attention_heads *\nsize_per_head]).</p>\n<p>Raises: ValueError: Any of the arguments or tensor shapes are\ninvalid. \"\"\"</p>\n<p>def transpose_for_scores(input_tensor, batch_size,\nnum_attention_heads, seq_length, width): output_tensor = tf.reshape(\ninput_tensor, [batch_size, seq_length, num_attention_heads, width])</p>\n<pre><code>output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\nreturn output_tensor</code></pre>\n<p>from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\nto_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</p>\n<p>if len(from_shape) != len(to_shape): raise ValueError( \"The rank of\n<code>from_tensor</code> must match the rank of\n<code>to_tensor</code>.\")</p>\n<p>if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length =\nfrom_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if\n(batch_size is None or from_seq_length is None or to_seq_length is\nNone): raise ValueError( \"When passing in rank 2 tensors to\nattention_layer, the values \" \"for <code>batch_size</code>,\n<code>from_seq_length</code>, and <code>to_seq_length</code> \" \"must all\nbe specified.\")</p>\n<p># Scalar dimensions referenced here: # B = batch size (number of\nsequences) # F = <code>from_tensor</code> sequence length # T =\n<code>to_tensor</code> sequence length # N =\n<code>num_attention_heads</code> # H = <code>size_per_head</code></p>\n<p>from_tensor_2d = reshape_to_matrix(from_tensor) to_tensor_2d =\nreshape_to_matrix(to_tensor)</p>\n<p># <code>query_layer</code> = [B<em>F, N</em>H] query_layer =\ntf.compat.v1.layers.dense( from_tensor_2d, num_attention_heads *\nsize_per_head, activation=query_act, name=\"query\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>key_layer</code> = [B<em>T, N</em>H] key_layer =\ntf.compat.v1.layers.dense( to_tensor_2d, num_attention_heads *\nsize_per_head, activation=key_act, name=\"key\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>value_layer</code> = [B<em>T, N</em>H] value_layer =\ntf.compat.v1.layers.dense( to_tensor_2d, num_attention_heads *\nsize_per_head, activation=value_act, name=\"value\",\nkernel_initializer=create_initializer(initializer_range))</p>\n<p># <code>query_layer</code> = [B, N, F, H] query_layer =\ntranspose_for_scores(query_layer, batch_size, num_attention_heads,\nfrom_seq_length, size_per_head)</p>\n<p># <code>key_layer</code> = [B, N, T, H] key_layer =\ntranspose_for_scores(key_layer, batch_size, num_attention_heads,\nto_seq_length, size_per_head)</p>\n<p># Take the dot product between \"query\" and \"key\" to get the raw #\nattention scores. # <code>attention_scores</code> = [B, N, F, T]\nattention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\nattention_scores = tf.multiply(attention_scores, 1.0 /\nmath.sqrt(float(size_per_head)))</p>\n<p>if attention_mask is not None: # <code>attention_mask</code> = [B, 1,\nF, T] attention_mask = tf.expand_dims(attention_mask, axis=[1])</p>\n<pre><code># Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n# masked positions, this operation will create a tensor which is 0.0 for\n# positions we want to attend and -10000.0 for masked positions.\nadder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n# Since we are adding it to the raw scores before the softmax, this is\n# effectively the same as removing these entirely.\nattention_scores += adder</code></pre>\n<p># Normalize the attention scores to probabilities. #\n<code>attention_probs</code> = [B, N, F, T] attention_probs =\ntf.nn.softmax(attention_scores)</p>\n<p># This is actually dropping out entire tokens to attend to, which\nmight # seem a bit unusual, but is taken from the original Transformer\npaper. attention_probs = dropout(attention_probs,\nattention_probs_dropout_prob)</p>\n<p># <code>value_layer</code> = [B, T, N, H] value_layer = tf.reshape(\nvalue_layer, [batch_size, to_seq_length, num_attention_heads,\nsize_per_head])</p>\n<p># <code>value_layer</code> = [B, N, T, H] value_layer =\ntf.transpose(value_layer, [0, 2, 1, 3])</p>\n<p># <code>context_layer</code> = [B, N, F, H] context_layer =\ntf.matmul(attention_probs, value_layer)</p>\n<p># <code>context_layer</code> = [B, F, N, H] context_layer =\ntf.transpose(context_layer, [0, 2, 1, 3])</p>\n<p>if do_return_2d_tensor: # <code>context_layer</code> = [B<em>F,\nN</em>H] context_layer = tf.reshape( context_layer, [batch_size *\nfrom_seq_length, num_attention_heads * size_per_head]) else: #\n<code>context_layer</code> = [B, F, N*H] context_layer = tf.reshape(\ncontext_layer, [batch_size, from_seq_length, num_attention_heads *\nsize_per_head])</p>\n<p>return context_layer</p>\n<p>def transformer_model(input_tensor, attention_mask=None,\nhidden_size=768, num_hidden_layers=12, num_attention_heads=12,\nintermediate_size=3072, intermediate_act_fn=gelu,\nhidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1,\ninitializer_range=0.02, do_return_all_layers=False): \"\"\"Multi-headed,\nmulti-layer Transformer from \"Attention is All You Need\".</p>\n<p>This is almost an exact implementation of the original Transformer\nencoder.</p>\n<p>See the original paper: https://arxiv.org/abs/1706.03762</p>\n<p>Also see:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</p>\n<p>Args: input_tensor: float Tensor of shape [batch_size, seq_length,\nhidden_size]. attention_mask: (optional) int32 Tensor of shape\n[batch_size, seq_length, seq_length], with 1 for positions that can be\nattended to and 0 in positions that should not be. hidden_size: int.\nHidden size of the Transformer. num_hidden_layers: int. Number of layers\n(blocks) in the Transformer. num_attention_heads: int. Number of\nattention heads in the Transformer. intermediate_size: int. The size of\nthe \"intermediate\" (a.k.a., feed forward) layer. intermediate_act_fn:\nfunction. The non-linear activation function to apply to the output of\nthe intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout\nprobability for the hidden layers. attention_probs_dropout_prob: float.\nDropout probability of the attention probabilities. initializer_range:\nfloat. Range of the initializer (stddev of truncated normal).\ndo_return_all_layers: Whether to also return all layers or just the\nfinal layer.</p>\n<p>Returns: float Tensor of shape [batch_size, seq_length, hidden_size],\nthe final hidden layer of the Transformer.</p>\n<p>Raises: ValueError: A Tensor shape or parameter is invalid. \"\"\" if\nhidden_size % num_attention_heads != 0: raise ValueError( \"The hidden\nsize (%d) is not a multiple of the number of attention \" \"heads (%d)\" %\n(hidden_size, num_attention_heads))</p>\n<p>attention_head_size = int(hidden_size / num_attention_heads)\ninput_shape = get_shape_list(input_tensor, expected_rank=3) batch_size =\ninput_shape[0] seq_length = input_shape[1] input_width =\ninput_shape[2]</p>\n<p># The Transformer performs sum residuals on all layers so the input\nneeds # to be the same as the hidden size. if input_width !=\nhidden_size: raise ValueError(\"The width of the input tensor (%d) !=\nhidden size (%d)\" % (input_width, hidden_size))</p>\n<p># We keep the representation as a 2D tensor to avoid re-shaping it\nback and # forth from a 3D tensor to a 2D tensor. Re-shapes are normally\nfree on # the GPU/CPU but may not be free on the TPU, so we want to\nminimize them to # help the optimizer. prev_output =\nreshape_to_matrix(input_tensor)</p>\n<p>all_layer_outputs = [] for layer_idx in range(num_hidden_layers):\nwith tf.compat.v1.variable_scope(\"layer_%d\" % layer_idx): layer_input =\nprev_output</p>\n<pre><code>  with tf.compat.v1.variable_scope(&quot;attention&quot;):\n    attention_heads = []\n    with tf.compat.v1.variable_scope(&quot;self&quot;):\n      attention_head = attention_layer(\n          from_tensor=layer_input,\n          to_tensor=layer_input,\n          attention_mask=attention_mask,\n          num_attention_heads=num_attention_heads,\n          size_per_head=attention_head_size,\n          attention_probs_dropout_prob=attention_probs_dropout_prob,\n          initializer_range=initializer_range,\n          do_return_2d_tensor=True,\n          batch_size=batch_size,\n          from_seq_length=seq_length,\n          to_seq_length=seq_length)\n      attention_heads.append(attention_head)\n\n    attention_output = None\n    if len(attention_heads) == 1:\n      attention_output = attention_heads[0]\n    else:\n      # In the case where we have other sequences, we just concatenate\n      # them to the self-attention head before the projection.\n      attention_output = tf.concat(attention_heads, axis=-1)\n\n    # Run a linear projection of `hidden_size` then add a residual\n    # with `layer_input`.\n    with tf.compat.v1.variable_scope(&quot;output&quot;):\n      attention_output = tf.compat.v1.layers.dense(\n          attention_output,\n          hidden_size,\n          kernel_initializer=create_initializer(initializer_range))\n      attention_output = dropout(attention_output, hidden_dropout_prob)\n      attention_output = layer_norm(attention_output + layer_input)\n\n  # The activation is only applied to the &quot;intermediate&quot; hidden layer.\n  with tf.compat.v1.variable_scope(&quot;intermediate&quot;):\n    intermediate_output = tf.compat.v1.layers.dense(\n        attention_output,\n        intermediate_size,\n        activation=intermediate_act_fn,\n        kernel_initializer=create_initializer(initializer_range))\n\n  # Down-project back to `hidden_size` then add the residual.\n  with tf.compat.v1.variable_scope(&quot;output&quot;):\n    layer_output = tf.compat.v1.layers.dense(\n        intermediate_output,\n        hidden_size,\n        kernel_initializer=create_initializer(initializer_range))\n    layer_output = dropout(layer_output, hidden_dropout_prob)\n    layer_output = layer_norm(layer_output + attention_output)\n    prev_output = layer_output\n    all_layer_outputs.append(layer_output)</code></pre>\n<p>if do_return_all_layers: final_outputs = [] for layer_output in\nall_layer_outputs: final_output = reshape_from_matrix(layer_output,\ninput_shape) final_outputs.append(final_output) return final_outputs\nelse: final_output = reshape_from_matrix(prev_output, input_shape)\nreturn final_output</p>\n<p>def get_shape_list(tensor, expected_rank=None, name=None): \"\"\"Returns\na list of the shape of tensor, preferring static dimensions.</p>\n<p>Args: tensor: A tf.Tensor object to find the shape of. expected_rank:\n(optional) int. The expected rank of <code>tensor</code>. If this is\nspecified and the <code>tensor</code> has a different rank, and\nexception will be thrown. name: Optional name of the tensor for the\nerror message.</p>\n<p>Returns: A list of dimensions of the shape of tensor. All static\ndimensions will be returned as python integers, and dynamic dimensions\nwill be returned as tf.Tensor scalars. \"\"\" if name is None: name =\ntensor.name</p>\n<p>if expected_rank is not None: assert_rank(tensor, expected_rank,\nname)</p>\n<p>shape = tensor.shape.as_list()</p>\n<p>non_static_indexes = [] for (index, dim) in enumerate(shape): if dim\nis None: non_static_indexes.append(index)</p>\n<p>if not non_static_indexes: return shape</p>\n<p>dyn_shape = tf.shape(tensor) for index in non_static_indexes:\nshape[index] = dyn_shape[index] return shape</p>\n<p>def reshape_to_matrix(input_tensor): \"\"\"Reshapes a &gt;= rank 2\ntensor to a rank 2 tensor (i.e., a matrix).\"\"\" ndims =\ninput_tensor.shape.ndims if ndims &lt; 2: raise ValueError(\"Input tensor\nmust have at least rank 2. Shape = %s\" % (input_tensor.shape)) if ndims\n== 2: return input_tensor</p>\n<p>width = input_tensor.shape[-1] output_tensor =\ntf.reshape(input_tensor, [-1, width]) return output_tensor</p>\n<p>def reshape_from_matrix(output_tensor, orig_shape_list): \"\"\"Reshapes\na rank 2 tensor back to its original rank &gt;= 2 tensor.\"\"\" if\nlen(orig_shape_list) == 2: return output_tensor</p>\n<p>output_shape = get_shape_list(output_tensor)</p>\n<p>orig_dims = orig_shape_list[0:-1] width = output_shape[-1]</p>\n<p>return tf.reshape(output_tensor, orig_dims + [width])</p>\n<p>def assert_rank(tensor, expected_rank, name=None): \"\"\"Raises an\nexception if the tensor rank is not of the expected rank.</p>\n<p>Args: tensor: A tf.Tensor to check the rank of. expected_rank: Python\ninteger or list of integers, expected rank. name: Optional name of the\ntensor for the error message.</p>\n<p>Raises: ValueError: If the expected shape doesn't match the actual\nshape. \"\"\" if name is None: name = tensor.name</p>\n<p>expected_rank_dict = {} if isinstance(expected_rank,\nsix.integer_types): expected_rank_dict[expected_rank] = True else: for x\nin expected_rank: expected_rank_dict[x] = True</p>\n<p>actual_rank = tensor.shape.ndims if actual_rank not in\nexpected_rank_dict: scope_name = tf.compat.v1.get_variable_scope().name\nraise ValueError( \"For the tensor <code>%s</code> in scope\n<code>%s</code>, the actual rank \" \"<code>%d</code> (shape = %s) is not\nequal to the expected rank <code>%s</code>\" % (name, scope_name,\nactual_rank, str(tensor.shape), str(expected_rank)))</p>\n<p>```</p>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","categories":[{"name":"Artificial Intelligence","path":"api/categories/Artificial Intelligence.json"},{"name":"Natural Language Processing","path":"api/categories/Natural Language Processing.json"},{"name":"Bert","path":"api/categories/Bert.json"},{"name":"Tensorflow 2.6","path":"api/categories/Tensorflow 2.6.json"}],"tags":[{"name":"Artificial Intelligence","path":"api/tags/Artificial Intelligence.json"},{"name":"Bert","path":"api/tags/Bert.json"},{"name":"Natural Language Processing","path":"api/tags/Natural Language Processing.json"},{"name":"Tensorflow 2.6","path":"api/tags/Tensorflow 2.6.json"}]}