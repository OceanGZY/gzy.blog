{"title":"CNN、RNN、DNN区别","slug":"CNN、RNN、DNN区别","date":"2019-01-01T15:02:00.000Z","updated":"2022-09-30T06:56:37.155Z","comments":true,"path":"api/articles/CNN、RNN、DNN区别.json","excerpt":null,"covers":["https://img-blog.csdnimg.cn/201811232052525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/20181123204831338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/20181123205052191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70","https://oceaneyes.top/img/zhishigroup.jpg","https://oceaneyes.top/img/alipay.jpg","https://oceaneyes.top/img/wechatpay.jpg"],"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"cnnrnndnn区别\">CNN、RNN、DNN区别</h2>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/201811232052525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\">\n<figcaption aria-hidden=\"true\">img</figcaption>\n</figure>\n<h3 id=\"神经网络起源\">神经网络起源</h3>\n<p><strong>感知机（perception），包含输入层，输出层，和一个隐藏层；</strong></p>\n<p>​ 输入的特征向量通过隐藏层变换达到输出层，由输出层得到分类结果。</p>\n<p><strong>多层感知机，多个隐藏层；</strong></p>\n<p><strong>神经网络NN，使用连续函数模拟神经元对激励的响应，在训练算法上使用反向传播算法；</strong></p>\n<p>1、神经网络的层数直接决定了它对现实的刻画能力----利用每层更少的神经元拟合更加附加的函数。</p>\n<p>2、神经网络层数加深，优化函数越容易陷入局部最优解，并且越来越偏离全局最优</p>\n<p>3、利用有限的数据训练深层的网络，性能还不如浅层网络</p>\n<p>4、网络层数的不断增加，“梯度消失”的现象更严重</p>\n<h3 id=\"dnn\">DNN</h3>\n<p>2006年，Hition提出深度学习，利用预训练的方式缓解局部最优解问题，将隐藏层增加到了7层</p>\n<h3 id=\"cnn-卷积神经网络convolutional-neural-networks\">CNN\n卷积神经网络Convolutional Neural Networks</h3>\n<p>图像中存在固有的局部模式，故将图像处理和神经网络结合引出卷机神经网络CNN（Convolutional\nNeural Networks），通过卷积核将上下层进行链接<img src=\"https://img-blog.csdnimg.cn/20181123204831338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\"></p>\n<p>因每层神经元的信号智能向上一层传播，样本的处理每个时刻独立，又被称为前向神经网络（Feed-forward\nNeural Networks）。</p>\n<h3 id=\"rnn-递归神经网络recursive-neural-network\">RNN\n递归神经网络Recursive Neural Network</h3>\n<p><strong>广义上分为：</strong></p>\n<ul>\n<li><p>结构递归神经网络</p></li>\n<li><p>时间递归神经网络</p></li>\n</ul>\n<p><strong>狭义上分为：</strong></p>\n<ul>\n<li>递归神经网络常常指的是 结构递归神经网络</li>\n<li>时间递归神经网络则成为 循环神经网络（Recurrent Neural Network）</li>\n</ul>\n<p><strong>循环神经网络</strong></p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20181123205052191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\">\n<figcaption aria-hidden=\"true\">img</figcaption>\n</figure>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","more":"<h2 id=\"cnnrnndnn区别\">CNN、RNN、DNN区别</h2>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/201811232052525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\">\n<figcaption aria-hidden=\"true\">img</figcaption>\n</figure>\n<h3 id=\"神经网络起源\">神经网络起源</h3>\n<p><strong>感知机（perception），包含输入层，输出层，和一个隐藏层；</strong></p>\n<p>​ 输入的特征向量通过隐藏层变换达到输出层，由输出层得到分类结果。</p>\n<p><strong>多层感知机，多个隐藏层；</strong></p>\n<p><strong>神经网络NN，使用连续函数模拟神经元对激励的响应，在训练算法上使用反向传播算法；</strong></p>\n<p>1、神经网络的层数直接决定了它对现实的刻画能力----利用每层更少的神经元拟合更加附加的函数。</p>\n<p>2、神经网络层数加深，优化函数越容易陷入局部最优解，并且越来越偏离全局最优</p>\n<p>3、利用有限的数据训练深层的网络，性能还不如浅层网络</p>\n<p>4、网络层数的不断增加，“梯度消失”的现象更严重</p>\n<h3 id=\"dnn\">DNN</h3>\n<p>2006年，Hition提出深度学习，利用预训练的方式缓解局部最优解问题，将隐藏层增加到了7层</p>\n<h3 id=\"cnn-卷积神经网络convolutional-neural-networks\">CNN\n卷积神经网络Convolutional Neural Networks</h3>\n<p>图像中存在固有的局部模式，故将图像处理和神经网络结合引出卷机神经网络CNN（Convolutional\nNeural Networks），通过卷积核将上下层进行链接<img src=\"https://img-blog.csdnimg.cn/20181123204831338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\"></p>\n<p>因每层神经元的信号智能向上一层传播，样本的处理每个时刻独立，又被称为前向神经网络（Feed-forward\nNeural Networks）。</p>\n<h3 id=\"rnn-递归神经网络recursive-neural-network\">RNN\n递归神经网络Recursive Neural Network</h3>\n<p><strong>广义上分为：</strong></p>\n<ul>\n<li><p>结构递归神经网络</p></li>\n<li><p>时间递归神经网络</p></li>\n</ul>\n<p><strong>狭义上分为：</strong></p>\n<ul>\n<li>递归神经网络常常指的是 结构递归神经网络</li>\n<li>时间递归神经网络则成为 循环神经网络（Recurrent Neural Network）</li>\n</ul>\n<p><strong>循环神经网络</strong></p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20181123205052191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnl1c2tp,size_16,color_FFFFFF,t_70\" alt=\"img\">\n<figcaption aria-hidden=\"true\">img</figcaption>\n</figure>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","categories":[{"name":"Artificial Intelligence","path":"api/categories/Artificial Intelligence.json"},{"name":"神经网络","path":"api/categories/神经网络.json"}],"tags":[{"name":"Artificial Intelligence","path":"api/tags/Artificial Intelligence.json"},{"name":"神经网络","path":"api/tags/神经网络.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"DNN","path":"api/tags/DNN.json"}]}