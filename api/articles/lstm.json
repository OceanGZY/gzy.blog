{"title":"LSTM","slug":"lstm","date":"2021-03-01T15:19:00.000Z","updated":"2022-09-30T06:56:37.176Z","comments":true,"path":"api/articles/lstm.json","excerpt":null,"covers":["https://camo.githubusercontent.com/b463c73dc5cb2fcd7c356c0aa1501cd44065cc7bf018ddc1e2ddc5252bb4b70d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165616464353863386136356639622e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/6f33cfec65bf836ce91e9040c5bff43defca9965eb37edf6911f17ba099176f7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d316266373331313061656562306634642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/5e1105f1d2a1295456d6f50bd8bdf0618ed1671c62438e2c40a2591ff59a18db/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373366626538303433643163313034342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/03e8e57928e72ed61c2d05964ffcbbb3cc56537dbf77cd399b012e47663880c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386665373562353932653064373764662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/87785eeb21f6319c756acfbbd10fe9428e35477395c4f81ff97fbe3ad25af32b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326634346233633737636635306537632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/edc2645e0f370d37a8e711ecce87f1b7205a02d39fbe97f92fcf338d7ba93b42/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626331383535363034383664613965662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/8112de4e3ca5b8d779b7dda15f8fec8fae31d243571b64e51347cb0fa53073bf/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303232316333653932333839336337302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/760a23e3bc9d0e7b39f038ab61156c47d701b46a52cd55fe573f1fe0b70605e1/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356432616433653735373133346362382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/915e8fc4731ac7dec46dac3a0561bcac86ea8805bca90031b1d85a1e528e6396/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303639376232356637663732616363642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://camo.githubusercontent.com/561bc8e2c4c87fefecb71aecb1e70b6119afcdabc817546ac6fd26b54fa44795/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633837383135646665643364663666362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430","https://oceaneyes.top/img/zhishigroup.jpg","https://oceaneyes.top/img/alipay.jpg","https://oceaneyes.top/img/wechatpay.jpg"],"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"lstm原理\">LSTM原理</h2>\n<h2 id=\"一基础介绍\">一、基础介绍</h2>\n<h3 id=\"神经网络模型\">1.1 神经网络模型</h3>\n<p>简单来说，常见的神经网络模型结构有前馈神经网络(DNN)、RNN（常用于文本\n/ 时间系列任务）、CNN（常用于图像任务）等等。具体可以看之前文章：<a href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fovx_lj2rCrrTx8DeU03yjQ\">一文概览神经网络模型。</a></p>\n<p>前馈神经网络是神经网络模型中最为常见的，信息从输入层开始输入，每层的神经元接收前一级输入，并输出到下一级，直至输出层。整个网络信息输入传输中无反馈（循环）。即任何层的输出都不会影响同级层，可用一个有向无环图表示。\n<a href=\"https://camo.githubusercontent.com/b463c73dc5cb2fcd7c356c0aa1501cd44065cc7bf018ddc1e2ddc5252bb4b70d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165616464353863386136356639622e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/b463c73dc5cb2fcd7c356c0aa1501cd44065cc7bf018ddc1e2ddc5252bb4b70d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165616464353863386136356639622e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"rnn-介绍\">1.2 RNN 介绍</h3>\n<p>循环神经网络（RNN）是基于序列数据（如语言、语音、时间序列）的递归性质而设计的，是一种反馈类型的神经网络，它专门用于处理序列数据，如逐字生成文本或预测时间序列数据(例如股票价格、诗歌生成)。\n<a href=\"https://camo.githubusercontent.com/6f33cfec65bf836ce91e9040c5bff43defca9965eb37edf6911f17ba099176f7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d316266373331313061656562306634642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/6f33cfec65bf836ce91e9040c5bff43defca9965eb37edf6911f17ba099176f7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d316266373331313061656562306634642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>RNN和全连接神经网络的本质差异在于“输入是带有反馈信息的”，RNN除了接受每一步的输入x(t)\n，同时还有输入上一步的历史反馈信息——隐藏状态h (t-1)\n，也就是当前时刻的隐藏状态h(t) 或决策输出O(t) 由当前时刻的输入 x(t)\n和上一时刻的隐藏状态h (t-1)\n共同决定。从某种程度，RNN和大脑的决策很像，大脑接受当前时刻感官到的信息（外部的x(t)\n）和之前的想法（内部的h (t-1) ）的输入一起决策。</p>\n<p><a href=\"https://camo.githubusercontent.com/5e1105f1d2a1295456d6f50bd8bdf0618ed1671c62438e2c40a2591ff59a18db/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373366626538303433643163313034342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/5e1105f1d2a1295456d6f50bd8bdf0618ed1671c62438e2c40a2591ff59a18db/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373366626538303433643163313034342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>RNN的结构原理可以简要概述为两个公式，具体介绍可以看下<a href=\"http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&amp;mid=2247486492&amp;idx=1&amp;sn=46c4391755acaf19607fe3ddd3d7b70a&amp;scene=19#wechat_redirect\">【一文详解RNN】</a>：</p>\n<blockquote>\n<p>RNN的隐藏状态为：h(t) = f( U * x(t) + W * h(t-1) + b1)，\nf为激活函数，常用tanh、relu; RNN的输出为：o(t) = g( V * h(t) +\nb2)，g为激活函数，当用于分类任务，一般用softmax;</p>\n</blockquote>\n<h3 id=\"从rnn到lstm\">1.3 从RNN到LSTM</h3>\n<p>但是在实际中，RNN在长序列数据处理中，容易导致梯度爆炸或者梯度消失，也就是长期依赖（long-term\ndependencies）问题，其根本原因就是模型“记忆”的序列信息太长了，都会一股脑地记忆和学习，时间一长，就容易忘掉更早的信息（梯度消失）或者崩溃（梯度爆炸）。</p>\n<blockquote>\n<p>梯度消失：历史时间步的信息距离当前时间步越长，反馈的梯度信号就会越弱（甚至为0）的现象，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。\n改善措施：可以使用 ReLU 激活函数；门控RNN 如GRU、LSTM\n以改善梯度消失。</p>\n</blockquote>\n<blockquote>\n<p>梯度爆炸：网络层之间的梯度（值大于\n1）重复相乘导致的指数级增长会产生梯度爆炸，导致模型无法有效学习。\n改善措施：可以使用 梯度截断；引导信息流的正则化；ReLU 激活函数；门控RNN\n如GRU、LSTM（和普通 RNN 相比多经过了很多次导数都小于 1激活函数，因此\nLSTM 发生梯度爆炸的频率要低得多）以改善梯度爆炸。</p>\n</blockquote>\n<p>所以，如果我们能让 RNN\n在接受上一时刻的状态和当前时刻的输入时，有选择地记忆和遗忘一部分内容（或者说信息），问题就可以解决了。比如上上句话提及”我去考试了“，然后后面提及”我考试通过了“，那么在此之前说的”我去考试了“的内容就没那么重要，选择性地遗忘就好了。这也就是长短期记忆网络（Long\nShort-Term Memory， LSTM）的基本思想。</p>\n<h2 id=\"二lstm原理\">二、LSTM原理</h2>\n<p>LSTM是种特殊RNN网络，在RNN的基础上引入了“门控”的选择性机制，分别是遗忘门、输入门和输出门，从而有选择性地保留或删除信息，以能够<strong>较好地</strong>学习长期依赖关系。如下图RNN（上）\n对比 LSTM（下）：</p>\n<p><a href=\"https://camo.githubusercontent.com/03e8e57928e72ed61c2d05964ffcbbb3cc56537dbf77cd399b012e47663880c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386665373562353932653064373764662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/03e8e57928e72ed61c2d05964ffcbbb3cc56537dbf77cd399b012e47663880c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386665373562353932653064373764662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"lstm的核心\">2.1 LSTM的核心</h3>\n<p>在RNN基础上引入门控后的LSTM，结构看起来好复杂！但其实LSTM作为一种反馈神经网络，<strong>核心还是历史的隐藏状态信息的反馈</strong>，也就是下图的Ct：\n<a href=\"https://camo.githubusercontent.com/87785eeb21f6319c756acfbbd10fe9428e35477395c4f81ff97fbe3ad25af32b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326634346233633737636635306537632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/87785eeb21f6319c756acfbbd10fe9428e35477395c4f81ff97fbe3ad25af32b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326634346233633737636635306537632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a>\n对标RNN的ht隐藏状态的更新，<strong>LSTM的Ct只是多个些“门控”删除或添加信息到状态信息</strong>。由下面依次介绍LSTM的“门控”：遗忘门，输入门，输出门的​功能，LSTM的原理也就好理解了。</p>\n<h3 id=\"遗忘门\">2.2 遗忘门</h3>\n<p>LSTM 的第一步是通过\"遗忘门\"从上个时间点的状态Ct-1中丢弃哪些信息。</p>\n<p>具体来说，输入Ct-1，会先根据上一个时间点的输出ht-1和当前时间点的输入xt，并通过sigmoid激活函数的输出结果ft来确定要让Ct-1，来忘记多少，sigmoid后等于1表示要保存多一些Ct-1的比重，等于0表示完全忘记之前的Ct-1。\n<a href=\"https://camo.githubusercontent.com/edc2645e0f370d37a8e711ecce87f1b7205a02d39fbe97f92fcf338d7ba93b42/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626331383535363034383664613965662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/edc2645e0f370d37a8e711ecce87f1b7205a02d39fbe97f92fcf338d7ba93b42/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626331383535363034383664613965662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"输入门\">2.3 输入门</h3>\n<p>下一步是通过输入门，决定我们将在状态中存储哪些新信息。</p>\n<p>我们根据上一个时间点的输出ht-1和当前时间点的输入xt 生成两部分信息i t\n及C<del>t，通过sigmoid输出i t，用tanh输出C</del>t。之后通过把i t\n及C~t两个部分相乘，共同决定在状态中存储哪些新信息。 <a href=\"https://camo.githubusercontent.com/8112de4e3ca5b8d779b7dda15f8fec8fae31d243571b64e51347cb0fa53073bf/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303232316333653932333839336337302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/8112de4e3ca5b8d779b7dda15f8fec8fae31d243571b64e51347cb0fa53073bf/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303232316333653932333839336337302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>在输入门 + 遗忘门控制下，当前时间点状态信息Ct为：</p>\n<p><a href=\"https://camo.githubusercontent.com/760a23e3bc9d0e7b39f038ab61156c47d701b46a52cd55fe573f1fe0b70605e1/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356432616433653735373133346362382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/760a23e3bc9d0e7b39f038ab61156c47d701b46a52cd55fe573f1fe0b70605e1/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356432616433653735373133346362382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"输出门\">2.4 输出门</h3>\n<p>最后，我们根据上一个时间点的输出ht-1和当前时间点的输入xt 通过sigmid\n输出Ot，再根据Ot 与 tanh控制的当前时间点状态信息Ct 相乘作为最终的输出。\n<a href=\"https://camo.githubusercontent.com/915e8fc4731ac7dec46dac3a0561bcac86ea8805bca90031b1d85a1e528e6396/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303639376232356637663732616363642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/915e8fc4731ac7dec46dac3a0561bcac86ea8805bca90031b1d85a1e528e6396/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303639376232356637663732616363642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p><strong>综上，一张图可以说清LSTM原理：</strong> <a href=\"https://camo.githubusercontent.com/561bc8e2c4c87fefecb71aecb1e70b6119afcdabc817546ac6fd26b54fa44795/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633837383135646665643364663666362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/561bc8e2c4c87fefecb71aecb1e70b6119afcdabc817546ac6fd26b54fa44795/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633837383135646665643364663666362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","more":"<h2 id=\"lstm原理\">LSTM原理</h2>\n<h2 id=\"一基础介绍\">一、基础介绍</h2>\n<h3 id=\"神经网络模型\">1.1 神经网络模型</h3>\n<p>简单来说，常见的神经网络模型结构有前馈神经网络(DNN)、RNN（常用于文本\n/ 时间系列任务）、CNN（常用于图像任务）等等。具体可以看之前文章：<a href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fovx_lj2rCrrTx8DeU03yjQ\">一文概览神经网络模型。</a></p>\n<p>前馈神经网络是神经网络模型中最为常见的，信息从输入层开始输入，每层的神经元接收前一级输入，并输出到下一级，直至输出层。整个网络信息输入传输中无反馈（循环）。即任何层的输出都不会影响同级层，可用一个有向无环图表示。\n<a href=\"https://camo.githubusercontent.com/b463c73dc5cb2fcd7c356c0aa1501cd44065cc7bf018ddc1e2ddc5252bb4b70d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165616464353863386136356639622e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/b463c73dc5cb2fcd7c356c0aa1501cd44065cc7bf018ddc1e2ddc5252bb4b70d/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633165616464353863386136356639622e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"rnn-介绍\">1.2 RNN 介绍</h3>\n<p>循环神经网络（RNN）是基于序列数据（如语言、语音、时间序列）的递归性质而设计的，是一种反馈类型的神经网络，它专门用于处理序列数据，如逐字生成文本或预测时间序列数据(例如股票价格、诗歌生成)。\n<a href=\"https://camo.githubusercontent.com/6f33cfec65bf836ce91e9040c5bff43defca9965eb37edf6911f17ba099176f7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d316266373331313061656562306634642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/6f33cfec65bf836ce91e9040c5bff43defca9965eb37edf6911f17ba099176f7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d316266373331313061656562306634642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>RNN和全连接神经网络的本质差异在于“输入是带有反馈信息的”，RNN除了接受每一步的输入x(t)\n，同时还有输入上一步的历史反馈信息——隐藏状态h (t-1)\n，也就是当前时刻的隐藏状态h(t) 或决策输出O(t) 由当前时刻的输入 x(t)\n和上一时刻的隐藏状态h (t-1)\n共同决定。从某种程度，RNN和大脑的决策很像，大脑接受当前时刻感官到的信息（外部的x(t)\n）和之前的想法（内部的h (t-1) ）的输入一起决策。</p>\n<p><a href=\"https://camo.githubusercontent.com/5e1105f1d2a1295456d6f50bd8bdf0618ed1671c62438e2c40a2591ff59a18db/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373366626538303433643163313034342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/5e1105f1d2a1295456d6f50bd8bdf0618ed1671c62438e2c40a2591ff59a18db/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d373366626538303433643163313034342e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>RNN的结构原理可以简要概述为两个公式，具体介绍可以看下<a href=\"http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&amp;mid=2247486492&amp;idx=1&amp;sn=46c4391755acaf19607fe3ddd3d7b70a&amp;scene=19#wechat_redirect\">【一文详解RNN】</a>：</p>\n<blockquote>\n<p>RNN的隐藏状态为：h(t) = f( U * x(t) + W * h(t-1) + b1)，\nf为激活函数，常用tanh、relu; RNN的输出为：o(t) = g( V * h(t) +\nb2)，g为激活函数，当用于分类任务，一般用softmax;</p>\n</blockquote>\n<h3 id=\"从rnn到lstm\">1.3 从RNN到LSTM</h3>\n<p>但是在实际中，RNN在长序列数据处理中，容易导致梯度爆炸或者梯度消失，也就是长期依赖（long-term\ndependencies）问题，其根本原因就是模型“记忆”的序列信息太长了，都会一股脑地记忆和学习，时间一长，就容易忘掉更早的信息（梯度消失）或者崩溃（梯度爆炸）。</p>\n<blockquote>\n<p>梯度消失：历史时间步的信息距离当前时间步越长，反馈的梯度信号就会越弱（甚至为0）的现象，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。\n改善措施：可以使用 ReLU 激活函数；门控RNN 如GRU、LSTM\n以改善梯度消失。</p>\n</blockquote>\n<blockquote>\n<p>梯度爆炸：网络层之间的梯度（值大于\n1）重复相乘导致的指数级增长会产生梯度爆炸，导致模型无法有效学习。\n改善措施：可以使用 梯度截断；引导信息流的正则化；ReLU 激活函数；门控RNN\n如GRU、LSTM（和普通 RNN 相比多经过了很多次导数都小于 1激活函数，因此\nLSTM 发生梯度爆炸的频率要低得多）以改善梯度爆炸。</p>\n</blockquote>\n<p>所以，如果我们能让 RNN\n在接受上一时刻的状态和当前时刻的输入时，有选择地记忆和遗忘一部分内容（或者说信息），问题就可以解决了。比如上上句话提及”我去考试了“，然后后面提及”我考试通过了“，那么在此之前说的”我去考试了“的内容就没那么重要，选择性地遗忘就好了。这也就是长短期记忆网络（Long\nShort-Term Memory， LSTM）的基本思想。</p>\n<h2 id=\"二lstm原理\">二、LSTM原理</h2>\n<p>LSTM是种特殊RNN网络，在RNN的基础上引入了“门控”的选择性机制，分别是遗忘门、输入门和输出门，从而有选择性地保留或删除信息，以能够<strong>较好地</strong>学习长期依赖关系。如下图RNN（上）\n对比 LSTM（下）：</p>\n<p><a href=\"https://camo.githubusercontent.com/03e8e57928e72ed61c2d05964ffcbbb3cc56537dbf77cd399b012e47663880c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386665373562353932653064373764662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/03e8e57928e72ed61c2d05964ffcbbb3cc56537dbf77cd399b012e47663880c0/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d386665373562353932653064373764662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"lstm的核心\">2.1 LSTM的核心</h3>\n<p>在RNN基础上引入门控后的LSTM，结构看起来好复杂！但其实LSTM作为一种反馈神经网络，<strong>核心还是历史的隐藏状态信息的反馈</strong>，也就是下图的Ct：\n<a href=\"https://camo.githubusercontent.com/87785eeb21f6319c756acfbbd10fe9428e35477395c4f81ff97fbe3ad25af32b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326634346233633737636635306537632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/87785eeb21f6319c756acfbbd10fe9428e35477395c4f81ff97fbe3ad25af32b/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d326634346233633737636635306537632e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a>\n对标RNN的ht隐藏状态的更新，<strong>LSTM的Ct只是多个些“门控”删除或添加信息到状态信息</strong>。由下面依次介绍LSTM的“门控”：遗忘门，输入门，输出门的​功能，LSTM的原理也就好理解了。</p>\n<h3 id=\"遗忘门\">2.2 遗忘门</h3>\n<p>LSTM 的第一步是通过\"遗忘门\"从上个时间点的状态Ct-1中丢弃哪些信息。</p>\n<p>具体来说，输入Ct-1，会先根据上一个时间点的输出ht-1和当前时间点的输入xt，并通过sigmoid激活函数的输出结果ft来确定要让Ct-1，来忘记多少，sigmoid后等于1表示要保存多一些Ct-1的比重，等于0表示完全忘记之前的Ct-1。\n<a href=\"https://camo.githubusercontent.com/edc2645e0f370d37a8e711ecce87f1b7205a02d39fbe97f92fcf338d7ba93b42/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626331383535363034383664613965662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/edc2645e0f370d37a8e711ecce87f1b7205a02d39fbe97f92fcf338d7ba93b42/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d626331383535363034383664613965662e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"输入门\">2.3 输入门</h3>\n<p>下一步是通过输入门，决定我们将在状态中存储哪些新信息。</p>\n<p>我们根据上一个时间点的输出ht-1和当前时间点的输入xt 生成两部分信息i t\n及C<del>t，通过sigmoid输出i t，用tanh输出C</del>t。之后通过把i t\n及C~t两个部分相乘，共同决定在状态中存储哪些新信息。 <a href=\"https://camo.githubusercontent.com/8112de4e3ca5b8d779b7dda15f8fec8fae31d243571b64e51347cb0fa53073bf/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303232316333653932333839336337302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/8112de4e3ca5b8d779b7dda15f8fec8fae31d243571b64e51347cb0fa53073bf/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303232316333653932333839336337302e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p>在输入门 + 遗忘门控制下，当前时间点状态信息Ct为：</p>\n<p><a href=\"https://camo.githubusercontent.com/760a23e3bc9d0e7b39f038ab61156c47d701b46a52cd55fe573f1fe0b70605e1/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356432616433653735373133346362382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/760a23e3bc9d0e7b39f038ab61156c47d701b46a52cd55fe573f1fe0b70605e1/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d356432616433653735373133346362382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<h3 id=\"输出门\">2.4 输出门</h3>\n<p>最后，我们根据上一个时间点的输出ht-1和当前时间点的输入xt 通过sigmid\n输出Ot，再根据Ot 与 tanh控制的当前时间点状态信息Ct 相乘作为最终的输出。\n<a href=\"https://camo.githubusercontent.com/915e8fc4731ac7dec46dac3a0561bcac86ea8805bca90031b1d85a1e528e6396/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303639376232356637663732616363642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/915e8fc4731ac7dec46dac3a0561bcac86ea8805bca90031b1d85a1e528e6396/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d303639376232356637663732616363642e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<p><strong>综上，一张图可以说清LSTM原理：</strong> <a href=\"https://camo.githubusercontent.com/561bc8e2c4c87fefecb71aecb1e70b6119afcdabc817546ac6fd26b54fa44795/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633837383135646665643364663666362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\"><img src=\"https://camo.githubusercontent.com/561bc8e2c4c87fefecb71aecb1e70b6119afcdabc817546ac6fd26b54fa44795/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31313638323237312d633837383135646665643364663666362e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430\" alt=\"img\"></a></p>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","categories":[{"name":"Artificial Intelligence","path":"api/categories/Artificial Intelligence.json"},{"name":"Machine Learning","path":"api/categories/Machine Learning.json"},{"name":"Algorithm","path":"api/categories/Algorithm.json"}],"tags":[{"name":"Machine Learning","path":"api/tags/Machine Learning.json"},{"name":"Algorithm","path":"api/tags/Algorithm.json"},{"name":"LSTM","path":"api/tags/LSTM.json"}]}