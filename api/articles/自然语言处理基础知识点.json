{"title":"自然语言处理基础知识","slug":"自然语言处理基础知识点","date":"2019-12-28T15:02:00.000Z","updated":"2022-09-30T06:56:37.203Z","comments":true,"path":"api/articles/自然语言处理基础知识点.json","excerpt":null,"covers":["https://img-blog.csdnimg.cn/20190407193133441.gif","https://img-blog.csdnimg.cn/20190407193205893.gif","https://img-blog.csdnimg.cn/20190407193223473.gif","https://img-blog.csdnimg.cn/20190407193243788.gif","https://img-blog.csdnimg.cn/20190625094348755.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/20190625094424708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70","https://img-blog.csdnimg.cn/20190407193344547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70","https://oceaneyes.top/img/zhishigroup.jpg","https://oceaneyes.top/img/alipay.jpg","https://oceaneyes.top/img/wechatpay.jpg"],"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"natural-language-processing\">Natural Language Processing</h1>\n<h2 id=\"自然语言发展历程\">自然语言发展历程</h2>\n<ul>\n<li>谷歌翻译，2016年上线全新版本神经网络翻译系统，基于RNN</li>\n<li>Facebook的FAIR（Facebook AI Research）发布《<strong>Convolutional\nSequence to Sequence Learning</strong>》， 基于CNN的端到端训练</li>\n<li>谷歌2017年6月发布《Attention is All you Need》，提出Transformer模型\n<ul>\n<li>自注意力 self-attention</li>\n<li>多头注意力 multi-head attention</li>\n<li>位置嵌入 positional encoding</li>\n</ul></li>\n<li>OpenAI 2018年发表《Improving Language Understanding by Generative\nPre-Training》，提出GPT模型，基于transformer提取特征，跑12个任务，9个任务都达到了最佳\n<ul>\n<li>单向语言模型</li>\n</ul></li>\n<li>AllenAI 2018年8月发表《Deep contextualized word\nrepresentations》，提出模型ELMo\n<ul>\n<li>双向LSTM语言模型</li>\n</ul></li>\n<li>谷歌2018年10月，发表《Pre-training of Deep Bidirectional\nTransformers for Language Understanding》，提出BERT模型</li>\n<li>百度的ERINE模型</li>\n<li>OpenAI\n2019年2月提出GPT2.0，模型更深，训练数据更多，参数高达15亿</li>\n<li>微软提出MASS模型，（Masked Sequence to Sequence Pre-training）</li>\n<li>谷歌2019年6月提出XLNet</li>\n<li>Facebook2019年7月发表《RoBERTa: A Robustly Optimized BERT\nPretraining Approach》，提出RoBERTa模型</li>\n<li>……natural language processing</li>\n</ul>\n<p><strong>没有最强，只有更强。。。。</strong></p>\n<h2 id=\"自然语言处理的任务应用\">自然语言处理的任务应用</h2>\n<h3 id=\"句法语义分析\">句法语义分析</h3>\n<p>对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧</p>\n<h3 id=\"信息抽取\">信息抽取</h3>\n<p>从给定文本中抽取重要信息，比如抽取：</p>\n<ul>\n<li>时间</li>\n<li>地点</li>\n<li>人物</li>\n<li>事件</li>\n<li>原因</li>\n<li>结果</li>\n<li>数字</li>\n<li>日期</li>\n<li>货币</li>\n<li>专有名词</li>\n<li>……</li>\n</ul>\n<p>旨在了解谁在什么时候、什么原因、对谁\n、做了什么事、有什么结果，涉及到实体识别、时间抽取、因果关系抽取等关键技术</p>\n<h3 id=\"文本挖掘\">文本挖掘</h3>\n<p>文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识等可视化、交互式的表达界面</p>\n<h3 id=\"机器翻译\">机器翻译</h3>\n<p>将输入的源语言文本通过自动翻译获得另外一种语言的文本。</p>\n<p>根据输入媒介不同，细分为：</p>\n<ul>\n<li>文本翻译</li>\n<li>语音翻译</li>\n<li>手语翻译</li>\n<li>图形翻译</li>\n<li>……</li>\n</ul>\n<h3 id=\"信息检索\">信息检索</h3>\n<p>对大规模的文档进行索引，可简单的对文档中的词汇赋予不同的权重来建立索引，可利用1，2，3的技术来建立更深层的索引</p>\n<h3 id=\"问答系统\">问答系统</h3>\n<p>对一个自然语言表达的问题，由问答系统给出一个精准的答案</p>\n<p>需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别、形成逻辑表达式，然后在知识库中查找可能的候选答案并通过排序机制找出最佳答案</p>\n<h3 id=\"对话系统\">对话系统</h3>\n<p>系统通过一系列对话，跟用户进行聊天、回答、完成某一项任务</p>\n<p>涉及到用户意图的理解、通过聊天引擎、问答引擎、对话管理技术，同时为体现和保证上下文的关联，需要具备多轮对话的能力</p>\n<h2 id=\"自然语言处理模型\">自然语言处理模型</h2>\n<h3 id=\"seq2seq模型\">Seq2Seq模型</h3>\n<ul>\n<li>encoder层</li>\n<li>decoder层</li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193133441.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1、在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词语前一个节点的\nhidden state，最终encoder会输出一个context</p>\n<p>2、这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并将decoder的\nhidden state作为下一层的输入</p>\n<p>该模型对短文本的翻译而言效果较好，但存在一定的缺点，如果文本稍长，就容易丢失文本的一些信息</p>\n<h3 id=\"attention\">Attention</h3>\n<p>Attention是一种能让模型对重要信息进行重点关注并充分学习吸收的技术</p>\n<p>Attention注意力，该模型在decoder阶段，会选择最适合当前节点的context作为输入</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193205893.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1、encoder提供更多的数据给到decoder，encoder会把所有节点的hidden\nstate提供给decoder，而不仅仅是encoder的最后一个节点的hidden state</p>\n<p>2、decoder并不是直接吧所有的encoder提供的hidden\nstate作为输入，而是采取一种选择机制，把最符合当前位置的hidden\nstate选出来</p>\n<ul>\n<li>确定哪个hidden state与当前节点关系最为密切</li>\n<li>计算每一个hidden state的分值</li>\n<li>对每个分数值做一个softmax的计算，使得相关性高的hidden\nstate的分数值更大，相关性低的hidden state分数值更低</li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193223473.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1）把每一个encoder节点的hidden states的值\n与decoder当前节点的上一个节点的hidden state相乘，\n得到每个encoder节点的每个hidden state的分数</p>\n<p>2）将得到的分数进行softmax计算，计算之后的值即为每一个encoder节点的\nhidden states 对于当前节点的权重</p>\n<p>3）将权重与原hidden states相乘并相加，得到的结果即为当前节点的hidden\nstate</p>\n<h5 id=\"decoder层的工作原理\">decoder层的工作原理</h5>\n<ul>\n<li><p>第一个decoder的节点初始化一个向量，并计算当前节点的hidden\nstate</p></li>\n<li><p>将得到的hidden\nstate作为第一个节点的输入，经过RNN节点后得到一个新的hidden\nstate与输出值</p>\n<p>区别：</p>\n<p><strong>seq2seq</strong>是直接把输出值作为当前节点的输出</p>\n<p><strong>Attention</strong>是把该值与hidden\nstate做成一个链接，并把连接好的值作为context，送入一个前馈神经网络，最终当前节点的输出内容由该网络决定</p></li>\n<li><p>把连接好的值作为context，送入一个前馈神经网络，最终当前节点的输出内容由该网络决定</p></li>\n<li><p>重复以上步骤，直到把所有的 coder的节点都输出相应的内容</p></li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193243788.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>Attention模型并不只是盲目地将输出的第一个单词与输入的第一个词对齐，实际在训练阶段学习了如何在语言中对齐单词。</p>\n<p><strong>Attention函数的本质，可被描述为一个查询（query）到一系列（键key\n-值value）对的映射</strong></p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190625094348755.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<h6 id=\"计算attention的主要步骤\">计算attention的主要步骤</h6>\n<ul>\n<li>首先将query\n和每个key进行相似度计算得到权重，常用的相似度函数又点积，拼接，感知机等</li>\n<li>然后使用一个softmax函数对这些权重进行归一化</li>\n<li>最后将权重和相应的键值value进行加权求和，得到最后的attention</li>\n</ul>\n<p>目前在NLP研究中，key和value常常都是一个，key = value</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190625094424708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<h3 id=\"transformer模型\">Transformer模型</h3>\n<p><strong>《Attention is all you need》</strong></p>\n<p>1、不同于以往的主流机器翻译使用基于RNN的seq2seq模型框架，使用attention机制代替了RNN</p>\n<p>2、提出多头注意力（Multi-headed\nattention）机制方法，在编码器和解码器内大量使用多头自注意力机制（Multi-headed\nself-attent）</p>\n<h4 id=\"transformer总结结构\">Transformer总结结构</h4>\n<p>和Attention模型一样，Transformer也采用了encoder-的\ncoder架构，但其结构更加复杂，encoder层u由6个encoder组成，\ndecoder层也由6个decoder组成</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>每个encoder和decoder的简版结构</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<ul>\n<li>encoder\n<ul>\n<li>self-attention，帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义</li>\n<li>前馈神经网络</li>\n</ul></li>\n</ul>\n<p><strong>模型内部细节</strong></p>\n<p>1、首先对输入的数据进行 embedding，降维，可以理解为类似word 2 vector\n的操作</p>\n<p>2、embedding结束之后，输入到encoder层</p>\n<p>3、self-attention处理完数据之后把数据送给前馈神经网络</p>\n<p>4、前馈神经网络可并行进行计算</p>\n<p>5、将计算的输出 输入到下一个encoder</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193344547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<ul>\n<li>decoder\n<ul>\n<li>self-attention，帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义</li>\n<li>Encoder-Decoder\nAttention，帮助当前节点获取当前需要关注的重点内容</li>\n<li>前馈神经网络</li>\n</ul></li>\n</ul>\n<h4 id=\"self-attention\">self-attention</h4>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","more":"<h1 id=\"natural-language-processing\">Natural Language Processing</h1>\n<h2 id=\"自然语言发展历程\">自然语言发展历程</h2>\n<ul>\n<li>谷歌翻译，2016年上线全新版本神经网络翻译系统，基于RNN</li>\n<li>Facebook的FAIR（Facebook AI Research）发布《<strong>Convolutional\nSequence to Sequence Learning</strong>》， 基于CNN的端到端训练</li>\n<li>谷歌2017年6月发布《Attention is All you Need》，提出Transformer模型\n<ul>\n<li>自注意力 self-attention</li>\n<li>多头注意力 multi-head attention</li>\n<li>位置嵌入 positional encoding</li>\n</ul></li>\n<li>OpenAI 2018年发表《Improving Language Understanding by Generative\nPre-Training》，提出GPT模型，基于transformer提取特征，跑12个任务，9个任务都达到了最佳\n<ul>\n<li>单向语言模型</li>\n</ul></li>\n<li>AllenAI 2018年8月发表《Deep contextualized word\nrepresentations》，提出模型ELMo\n<ul>\n<li>双向LSTM语言模型</li>\n</ul></li>\n<li>谷歌2018年10月，发表《Pre-training of Deep Bidirectional\nTransformers for Language Understanding》，提出BERT模型</li>\n<li>百度的ERINE模型</li>\n<li>OpenAI\n2019年2月提出GPT2.0，模型更深，训练数据更多，参数高达15亿</li>\n<li>微软提出MASS模型，（Masked Sequence to Sequence Pre-training）</li>\n<li>谷歌2019年6月提出XLNet</li>\n<li>Facebook2019年7月发表《RoBERTa: A Robustly Optimized BERT\nPretraining Approach》，提出RoBERTa模型</li>\n<li>……natural language processing</li>\n</ul>\n<p><strong>没有最强，只有更强。。。。</strong></p>\n<h2 id=\"自然语言处理的任务应用\">自然语言处理的任务应用</h2>\n<h3 id=\"句法语义分析\">句法语义分析</h3>\n<p>对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧</p>\n<h3 id=\"信息抽取\">信息抽取</h3>\n<p>从给定文本中抽取重要信息，比如抽取：</p>\n<ul>\n<li>时间</li>\n<li>地点</li>\n<li>人物</li>\n<li>事件</li>\n<li>原因</li>\n<li>结果</li>\n<li>数字</li>\n<li>日期</li>\n<li>货币</li>\n<li>专有名词</li>\n<li>……</li>\n</ul>\n<p>旨在了解谁在什么时候、什么原因、对谁\n、做了什么事、有什么结果，涉及到实体识别、时间抽取、因果关系抽取等关键技术</p>\n<h3 id=\"文本挖掘\">文本挖掘</h3>\n<p>文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识等可视化、交互式的表达界面</p>\n<h3 id=\"机器翻译\">机器翻译</h3>\n<p>将输入的源语言文本通过自动翻译获得另外一种语言的文本。</p>\n<p>根据输入媒介不同，细分为：</p>\n<ul>\n<li>文本翻译</li>\n<li>语音翻译</li>\n<li>手语翻译</li>\n<li>图形翻译</li>\n<li>……</li>\n</ul>\n<h3 id=\"信息检索\">信息检索</h3>\n<p>对大规模的文档进行索引，可简单的对文档中的词汇赋予不同的权重来建立索引，可利用1，2，3的技术来建立更深层的索引</p>\n<h3 id=\"问答系统\">问答系统</h3>\n<p>对一个自然语言表达的问题，由问答系统给出一个精准的答案</p>\n<p>需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别、形成逻辑表达式，然后在知识库中查找可能的候选答案并通过排序机制找出最佳答案</p>\n<h3 id=\"对话系统\">对话系统</h3>\n<p>系统通过一系列对话，跟用户进行聊天、回答、完成某一项任务</p>\n<p>涉及到用户意图的理解、通过聊天引擎、问答引擎、对话管理技术，同时为体现和保证上下文的关联，需要具备多轮对话的能力</p>\n<h2 id=\"自然语言处理模型\">自然语言处理模型</h2>\n<h3 id=\"seq2seq模型\">Seq2Seq模型</h3>\n<ul>\n<li>encoder层</li>\n<li>decoder层</li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193133441.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1、在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词语前一个节点的\nhidden state，最终encoder会输出一个context</p>\n<p>2、这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并将decoder的\nhidden state作为下一层的输入</p>\n<p>该模型对短文本的翻译而言效果较好，但存在一定的缺点，如果文本稍长，就容易丢失文本的一些信息</p>\n<h3 id=\"attention\">Attention</h3>\n<p>Attention是一种能让模型对重要信息进行重点关注并充分学习吸收的技术</p>\n<p>Attention注意力，该模型在decoder阶段，会选择最适合当前节点的context作为输入</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193205893.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1、encoder提供更多的数据给到decoder，encoder会把所有节点的hidden\nstate提供给decoder，而不仅仅是encoder的最后一个节点的hidden state</p>\n<p>2、decoder并不是直接吧所有的encoder提供的hidden\nstate作为输入，而是采取一种选择机制，把最符合当前位置的hidden\nstate选出来</p>\n<ul>\n<li>确定哪个hidden state与当前节点关系最为密切</li>\n<li>计算每一个hidden state的分值</li>\n<li>对每个分数值做一个softmax的计算，使得相关性高的hidden\nstate的分数值更大，相关性低的hidden state分数值更低</li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193223473.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>1）把每一个encoder节点的hidden states的值\n与decoder当前节点的上一个节点的hidden state相乘，\n得到每个encoder节点的每个hidden state的分数</p>\n<p>2）将得到的分数进行softmax计算，计算之后的值即为每一个encoder节点的\nhidden states 对于当前节点的权重</p>\n<p>3）将权重与原hidden states相乘并相加，得到的结果即为当前节点的hidden\nstate</p>\n<h5 id=\"decoder层的工作原理\">decoder层的工作原理</h5>\n<ul>\n<li><p>第一个decoder的节点初始化一个向量，并计算当前节点的hidden\nstate</p></li>\n<li><p>将得到的hidden\nstate作为第一个节点的输入，经过RNN节点后得到一个新的hidden\nstate与输出值</p>\n<p>区别：</p>\n<p><strong>seq2seq</strong>是直接把输出值作为当前节点的输出</p>\n<p><strong>Attention</strong>是把该值与hidden\nstate做成一个链接，并把连接好的值作为context，送入一个前馈神经网络，最终当前节点的输出内容由该网络决定</p></li>\n<li><p>把连接好的值作为context，送入一个前馈神经网络，最终当前节点的输出内容由该网络决定</p></li>\n<li><p>重复以上步骤，直到把所有的 coder的节点都输出相应的内容</p></li>\n</ul>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193243788.gif\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>Attention模型并不只是盲目地将输出的第一个单词与输入的第一个词对齐，实际在训练阶段学习了如何在语言中对齐单词。</p>\n<p><strong>Attention函数的本质，可被描述为一个查询（query）到一系列（键key\n-值value）对的映射</strong></p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190625094348755.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<h6 id=\"计算attention的主要步骤\">计算attention的主要步骤</h6>\n<ul>\n<li>首先将query\n和每个key进行相似度计算得到权重，常用的相似度函数又点积，拼接，感知机等</li>\n<li>然后使用一个softmax函数对这些权重进行归一化</li>\n<li>最后将权重和相应的键值value进行加权求和，得到最后的attention</li>\n</ul>\n<p>目前在NLP研究中，key和value常常都是一个，key = value</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190625094424708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<h3 id=\"transformer模型\">Transformer模型</h3>\n<p><strong>《Attention is all you need》</strong></p>\n<p>1、不同于以往的主流机器翻译使用基于RNN的seq2seq模型框架，使用attention机制代替了RNN</p>\n<p>2、提出多头注意力（Multi-headed\nattention）机制方法，在编码器和解码器内大量使用多头自注意力机制（Multi-headed\nself-attent）</p>\n<h4 id=\"transformer总结结构\">Transformer总结结构</h4>\n<p>和Attention模型一样，Transformer也采用了encoder-的\ncoder架构，但其结构更加复杂，encoder层u由6个encoder组成，\ndecoder层也由6个decoder组成</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<p>每个encoder和decoder的简版结构</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<ul>\n<li>encoder\n<ul>\n<li>self-attention，帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义</li>\n<li>前馈神经网络</li>\n</ul></li>\n</ul>\n<p><strong>模型内部细节</strong></p>\n<p>1、首先对输入的数据进行 embedding，降维，可以理解为类似word 2 vector\n的操作</p>\n<p>2、embedding结束之后，输入到encoder层</p>\n<p>3、self-attention处理完数据之后把数据送给前馈神经网络</p>\n<p>4、前馈神经网络可并行进行计算</p>\n<p>5、将计算的输出 输入到下一个encoder</p>\n<figure>\n<img src=\"https://img-blog.csdnimg.cn/20190407193344547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\">\n<figcaption aria-hidden=\"true\">在这里插入图片描述</figcaption>\n</figure>\n<ul>\n<li>decoder\n<ul>\n<li>self-attention，帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义</li>\n<li>Encoder-Decoder\nAttention，帮助当前节点获取当前需要关注的重点内容</li>\n<li>前馈神经网络</li>\n</ul></li>\n</ul>\n<h4 id=\"self-attention\">self-attention</h4>\n<hr>\n<h3 id=\"about-me\">About ME</h3>\n<h5 id=\"读书城南-在未来面前我们都是孩子\">👋 读书城南，🤔\n在未来面前，我们都是孩子～</h5>\n<ul>\n<li>📙\n一个热衷于探索学习新方向、新事物的智能产品经理，闲暇时间喜欢coding💻、画图🎨、音乐🎵、学习ing~</li>\n</ul>\n<h5 id=\"social-media\">👋 Social Media</h5>\n<ul>\n<li><p>🛠️ Blog: <a href=\"http://oceaneyes.top\">http://oceaneyes.top</a></p></li>\n<li><p>⚡ PM导航: <a href=\"https://pmhub.oceangzy.top\">https://pmhub.oceangzy.top</a></p></li>\n<li><p>☘️ CNBLOG: <a href=\"https://www.cnblogs.com/oceaneyes-gzy/\">https://www.cnblogs.com/oceaneyes-gzy/</a></p></li>\n<li><p>🌱 AI PRJ自己部署的一些算法demo: <a href=\"http://ai.oceangzy.top/\">http://ai.oceangzy.top/</a></p></li>\n<li><p>📫 Email: 1450136519@qq.com</p></li>\n<li><p>💬 WeChat: <a href=\"https://oceaneyes.top/img/wechatqrcode.jpg\">OCEANGZY</a></p></li>\n<li><p>💬 公众号: <a href=\"https://oceaneyes.top/img/wechatgzh.jpeg\">UncleJoker-GZY</a></p></li>\n</ul>\n<h5 id=\"加入小组\">👋 加入小组~</h5>\n<p><img src=\"https://oceaneyes.top/img/zhishigroup.jpg\" title=\"加入组织\" alt width=\"240\"></p>\n<h5 id=\"感谢打赏\">👋 感谢打赏~</h5>\n<p><img src=\"https://oceaneyes.top/img/alipay.jpg\" title=\"支付宝打赏\" alt width=\"140\">\n<img src=\"https://oceaneyes.top/img/wechatpay.jpg\" title=\"微信打赏\" alt width=\"140\"></p>\n","categories":[{"name":"Artificial Intelligence","path":"api/categories/Artificial Intelligence.json"},{"name":"神经网络","path":"api/categories/神经网络.json"}],"tags":[{"name":"Artificial Intelligence","path":"api/tags/Artificial Intelligence.json"},{"name":"神经网络","path":"api/tags/神经网络.json"},{"name":"RNN","path":"api/tags/RNN.json"},{"name":"CNN","path":"api/tags/CNN.json"},{"name":"DNN","path":"api/tags/DNN.json"}]}